{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "# Introduction to Automation with LangChain, Generative AI, and Python\n",
    "**2.6: Conversation Buffer Window Memory**\n",
    "* Instructor: [Jeff Heaton](https://youtube.com/@HeatonResearch), WUSTL Center for Analytics and Business Insight (CABI), [Washington University in St. Louis](https://olin.wustl.edu/faculty-and-research/research-centers/center-for-analytics-and-business-insight/index.php)\n",
    "* For more information visit the [class website](https://github.com/jeffheaton/cabi_genai_automation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "We previously saw that we could build up an LLM chat client by building an ever-increasing script of what the human and AI said in the conversation. We constantly add the human response and wait to see what the AI will respond to next. This cycle continues as long as the chat.\n",
    "\n",
    "This ever-increasing chat memory is a typical pattern for LLMs, and as a result, LangChain has several predefined Python objects that allow you to implement this sort of memory-based chatbot.\n",
    "\n",
    "## Creating a Chat Conversation\n",
    "\n",
    "ConversationChain in LangChain is a framework designed to facilitate the development and management of conversational AI systems. It primarily orchestrates the interaction between different components, such as the dialogue management system, language models, and various types of memory that retain information from the conversation to improve response relevance and coherence.\n",
    "\n",
    "In this context, memory types play a crucial role. They help the system remember and utilize past interactions to maintain context and enhance the continuity of the conversation. This system frees us from tracking the conversation as performed manually in the pervious section.\n",
    "\n",
    "We first examine ConversationBufferWindowMemory is a dynamic memory model designed to manage the flow of a dialogue by retaining a record of the last K interactions within a conversation. This method ensures that the memory buffer maintains a manageable size, avoiding overflow and performance issues that can arise with an excessively large interaction history. By focusing on the most recent interactions, this memory type effectively creates a \"sliding window\" that continuously updates as new exchanges occur, allowing for contextually relevant responses while efficiently managing system resources. This approach is particularly beneficial in applications where maintaining an immediate and context-aware dialogue is crucial, such as in customer service bots or interactive learning tools.\n",
    "\n",
    "The provided code snippet demonstrates how to set up and use a conversational AI system using the LangChain framework, specifically focusing on maintaining a concise memory of recent interactions. The ConversationChain class orchestrates the conversation flow, integrating various components such as language model, memory, and prompt formatting.\n",
    "\n",
    "Firstly, several modules are imported, including ConversationChain for managing the conversation, ConversationBufferWindowMemory for handling the memory of recent interactions, and ChatOpenAI to interface with OpenAI's language models. The PromptTemplate is used to define the structure of the prompts sent to the language model.\n",
    "\n",
    "In the setup, MODEL specifies the particular version of the language model (GPT-3.5 Turbo), and TEMPLATE outlines the format of the conversation, integrating previous dialogue history and the current user input into the prompt. This structured prompt is then used to create a PromptTemplate object.\n",
    "\n",
    "The begin_conversation function initializes the language model and sets up the memory to store the last five interactions (k=5). This sliding window of memory helps keep the conversation relevant and efficient by focusing on the most recent exchanges. The ConversationChain object is created with the prompt template, the language model, and the memory buffer, and it's ready to process conversation inputs.\n",
    "\n",
    "The converse function takes a conversation object and a user prompt to produce responses. It uses the conversation object's invoke method, which applies the prompt template, consults the memory, and generates a response based on the current and recent dialogue context.\n",
    "\n",
    "Overall, this setup is optimized for creating a responsive and context-aware conversational AI that doesn't overload with too much past information, maintaining relevancy and efficiency in ongoing interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts.chat import PromptTemplate\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "MODEL = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "TEMPLATE = \"\"\"You are a helpful assistant. Format answers with markdown.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"history\", \"input\"], template=TEMPLATE)\n",
    "\n",
    "def begin_conversation():\n",
    "    # Initialize bedrock, use built in role\n",
    "    llm = ChatBedrock(\n",
    "        model_id=MODEL,\n",
    "        model_kwargs={\"temperature\": 0.7},\n",
    "    )\n",
    "\n",
    "    # Initialize memory and conversation\n",
    "    memory = ConversationBufferWindowMemory(k=5)\n",
    "    conversation = ConversationChain(\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return conversation\n",
    "\n",
    "def converse(conversation, prompt):\n",
    "    print(f\"***Human: {prompt}\")\n",
    "    output = conversation.invoke(prompt)\n",
    "    return output['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClPhLkGldhPt"
   },
   "source": [
    "We can now carry on a simple conversation with the LLM, using LangChain to track the conversation memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "ydaqwgRiH4D6",
    "outputId": "b9c6d85c-918f-48a8-bf61-dd49837d0337"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Hello, what is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Unfortunately, I don't actually know your name since we haven't met before. I'm Claude, an AI assistant created by Anthropic.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Oh sorry, my name is Jeff.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Nice to meet you, Jeff! I'm an AI assistant created by Anthropic to be helpful, honest and harmless. Please let me know if you have any questions or if there is anything I can assist you with.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Your name is Jeff.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize bedrock, use built in role\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL,\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "conversation = begin_conversation()\n",
    "output = converse(conversation, \"Hello, what is my name?\")\n",
    "display_markdown(output,raw=True)\n",
    "output = converse(conversation, \"Oh sorry, my name is Jeff.\")\n",
    "display_markdown(output,raw=True)\n",
    "output = converse(conversation, \"What is my name?\")\n",
    "display_markdown(output,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulYUrfVcnlgF"
   },
   "source": [
    "## Conversing with the LLM in Markdown\n",
    "\n",
    "Just as before, we can request that the LLM output be in mardown. This allows code and tables to be represented clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MHL6Ik8IM2PA"
   },
   "outputs": [],
   "source": [
    "def chat(conversation, prompt):\n",
    "  output = converse(conversation, prompt)\n",
    "  display_markdown(output,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvzsYl3Qplac"
   },
   "source": [
    "The provided code sequence demonstrates a conversation between a human user and a Large Language Model (LLM), making use of the chat function to interactively manage the conversation and display responses in Markdown format. This approach allows for a dynamic and contextually aware chat, while also enhancing the visual and structural presentation of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "PtLDak7TM_FU",
    "outputId": "74b1a787-f0bd-4add-c481-6bfe5603cb96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Unfortunately, I don't actually know your name. As an AI assistant created by Anthropic, I don't have personal information about you unless you provide it to me during our conversation.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Okay, then let me introduce myself, my name is Jeff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Nice to meet you Jeff! I've updated my knowledge that your name is Jeff. Please let me know if you have any other questions - I'm here to help.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Your name is Jeff, based on your introduction earlier in our conversation.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Give me a table of the 5 most populus cities with population and country.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Here is a table of the 5 most populous cities in the world with their population and country:\n",
       "\n",
       "| City | Population | Country |\n",
       "| --- | --- | --- |\n",
       "| Tokyo | 37,468,000 | Japan |\n",
       "| Delhi | 29,398,000 | India |\n",
       "| Mexico City | 21,581,000 | Mexico |\n",
       "| SÃ£o Paulo | 21,846,000 | Brazil |\n",
       "| Mumbai | 20,961,000 | India |\n",
       "```\n",
       "\n",
       "Note: The population figures are estimates for the metropolitan area populations as of 2022, and may vary slightly depending on the source."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = begin_conversation()\n",
    "chat(conversation, \"What is my name?\")\n",
    "chat(conversation, \"Okay, then let me introduce myself, my name is Jeff\")\n",
    "chat(conversation, \"What is my name?\")\n",
    "chat(conversation, \"Give me a table of the 5 most populus cities with population and country.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeY-9YSOno_t"
   },
   "source": [
    "## Constraining the Conversation with a System Prompt\n",
    "\n",
    "You can use the system prompt to constrain the conversation to a specific topic. Here, we provide a simple agent that will only discuss life insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "NyGpLJmWjGNq",
    "outputId": "9d2b3222-0ca4-434f-ab78-56f928bea144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm afraid I cannot provide your name, as I do not have access to personal information about you. As a helpful agent focused on life insurance, I can only provide general information related to life insurance policies and coverage. Please feel free to ask any questions you may have about life insurance."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Okay, then let me introduce myself, my name is Jeff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "I apologize, but as an AI assistant focused solely on providing helpful information about life insurance, I cannot engage in casual conversation or learn personal details about you. My role is to answer questions specifically related to life insurance products, policies, coverage, and related topics.\n",
       "\n",
       "If you have any questions pertaining to life insurance, I would be happy to assist you. Otherwise, I must refrain from further dialogue beyond the scope of life insurance information.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "As I mentioned before, I do not actually have access to personal information about you or any other users. My role is limited to providing helpful information specifically about life insurance products and policies.\n",
       "\n",
       "While I understand the desire to have a more personal interaction, I am an AI assistant created to focus solely on the topic of life insurance. I cannot learn, store or confirm any personal details like names.\n",
       "\n",
       "Please feel free to ask me any questions related to life insurance, and I will do my best to provide accurate and useful information within that scope. However, I cannot engage in casual conversation or confirm personal details outside of the life insurance domain.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is your favorite programming language?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "As an AI assistant focused on providing helpful information about life insurance, I do not actually have personal preferences or knowledge about programming languages. My role is to answer questions and provide information specifically related to life insurance products, policies, coverage options, and related topics within that domain.\n",
       "\n",
       "I apologize, but I cannot engage in discussions or answer questions outside of the life insurance subject area, as that is beyond the scope of my designed capabilities. Please feel free to ask me any other questions you may have pertaining to life insurance, and I will do my best to provide accurate and informative responses.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is the difference between a term and whole life policy?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "The primary difference between term life insurance and whole life insurance lies in the duration of coverage and the way the policies accumulate cash value.\n",
       "\n",
       "Term Life Insurance:\n",
       "- Provides temporary coverage for a specific time period, typically 10, 20, or 30 years\n",
       "- Offers only a death benefit with no cash value component\n",
       "- Premiums are generally lower than whole life insurance\n",
       "- Coverage ends at the end of the term unless the policy is renewed or converted\n",
       "\n",
       "Whole Life Insurance:\n",
       "- Provides lifelong coverage as long as premiums are paid\n",
       "- Builds up a cash value component that can be borrowed against or withdrawn\n",
       "- Premiums are typically higher than term life insurance\n",
       "- The cash value grows tax-deferred over time\n",
       "- Offers a guaranteed death benefit payout\n",
       "\n",
       "In summary, term life insurance provides temporary protection at a lower cost, while whole life insurance offers permanent coverage with a cash value component, but at a higher premium cost. The choice between the two depends on an individual's specific needs, budget, and long-term financial goals.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts.chat import PromptTemplate\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "def begin_conversation_insurance():\n",
    "    TEMPLATE = \"\"\"You are a helpful agent to answer questions about life insurance. Do not talk\n",
    "    about anything else with users. . Format answers with markdown.\n",
    "\n",
    "    Current conversation:\n",
    "    {history}\n",
    "    Human: {input}\n",
    "    AI:\"\"\"\n",
    "    PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"history\", \"input\"], template=TEMPLATE)\n",
    "\n",
    "    # Initialize bedrock, use built in role\n",
    "    llm = ChatBedrock(\n",
    "        model_id=MODEL,\n",
    "        model_kwargs={\"temperature\": 0.7},\n",
    "    )\n",
    "\n",
    "    # Initialize memory and conversation\n",
    "    memory = ConversationBufferWindowMemory(k=5)\n",
    "    conversation = ConversationChain(\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return conversation\n",
    "\n",
    "conversation = begin_conversation_insurance()\n",
    "chat(conversation, \"What is my name?\")\n",
    "chat(conversation, \"Okay, then let me introduce myself, my name is Jeff\")\n",
    "chat(conversation, \"What is my name?\")\n",
    "chat(conversation, \"What is your favorite programming language?\")\n",
    "chat(conversation, \"What is the difference between a term and whole life policy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgwRMwnlH5Ou"
   },
   "source": [
    "## Examining the Conversation Memory\n",
    "\n",
    "We can quickly look inside the memory of the LangChain-managed chat memory and see our conversation memory with the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZDN67SOHkNo",
    "outputId": "fe76bbba-c618-4773-94be-f1bea6a0b78d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: What is my name?\n",
      "AI: I'm afraid I cannot provide your name, as I do not have access to personal information about you. As a helpful agent focused on life insurance, I can only provide general information related to life insurance policies and coverage. Please feel free to ask any questions you may have about life insurance.\n",
      "Human: Okay, then let me introduce myself, my name is Jeff\n",
      "AI: ```\n",
      "I apologize, but as an AI assistant focused solely on providing helpful information about life insurance, I cannot engage in casual conversation or learn personal details about you. My role is to answer questions specifically related to life insurance products, policies, coverage, and related topics.\n",
      "\n",
      "If you have any questions pertaining to life insurance, I would be happy to assist you. Otherwise, I must refrain from further dialogue beyond the scope of life insurance information.\n",
      "```\n",
      "Human: What is my name?\n",
      "AI: ```\n",
      "As I mentioned before, I do not actually have access to personal information about you or any other users. My role is limited to providing helpful information specifically about life insurance products and policies.\n",
      "\n",
      "While I understand the desire to have a more personal interaction, I am an AI assistant created to focus solely on the topic of life insurance. I cannot learn, store or confirm any personal details like names.\n",
      "\n",
      "Please feel free to ask me any questions related to life insurance, and I will do my best to provide accurate and useful information within that scope. However, I cannot engage in casual conversation or confirm personal details outside of the life insurance domain.\n",
      "```\n",
      "Human: What is your favorite programming language?\n",
      "AI: ```\n",
      "As an AI assistant focused on providing helpful information about life insurance, I do not actually have personal preferences or knowledge about programming languages. My role is to answer questions and provide information specifically related to life insurance products, policies, coverage options, and related topics within that domain.\n",
      "\n",
      "I apologize, but I cannot engage in discussions or answer questions outside of the life insurance subject area, as that is beyond the scope of my designed capabilities. Please feel free to ask me any other questions you may have pertaining to life insurance, and I will do my best to provide accurate and informative responses.\n",
      "```\n",
      "Human: What is the difference between a term and whole life policy?\n",
      "AI: ```\n",
      "The primary difference between term life insurance and whole life insurance lies in the duration of coverage and the way the policies accumulate cash value.\n",
      "\n",
      "Term Life Insurance:\n",
      "- Provides temporary coverage for a specific time period, typically 10, 20, or 30 years\n",
      "- Offers only a death benefit with no cash value component\n",
      "- Premiums are generally lower than whole life insurance\n",
      "- Coverage ends at the end of the term unless the policy is renewed or converted\n",
      "\n",
      "Whole Life Insurance:\n",
      "- Provides lifelong coverage as long as premiums are paid\n",
      "- Builds up a cash value component that can be borrowed against or withdrawn\n",
      "- Premiums are typically higher than term life insurance\n",
      "- The cash value grows tax-deferred over time\n",
      "- Offers a guaranteed death benefit payout\n",
      "\n",
      "In summary, term life insurance provides temporary protection at a lower cost, while whole life insurance offers permanent coverage with a cash value component, but at a higher premium cost. The choice between the two depends on an individual's specific needs, budget, and long-term financial goals.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aW0TPWRBJ2w2"
   },
   "source": [
    "## Overloading the Memory\n",
    "\n",
    "When the conversation memory becomes full, the chatbot will begin to forget. For the ConversationBufferWindowMemory memory type, the oldest history will first be lost; there is no notion of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 820
    },
    "id": "d-a2iUpTJIYn",
    "outputId": "689496ee-16e2-44c4-fef4-30078d269b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Okay, then let me introduce myself, my name is Jeff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Nice to meet you, Jeff! I'm Claude, an AI assistant created by Anthropic. I'm here to help with any questions or tasks you may have. Please let me know if there's anything specific I can assist you with.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You have ONE JOB! Remember that my favorite color is blue.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Understood, Jeff! I will remember that your favorite color is blue.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Do you remember my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Yes, Jeff, I remember that your name is Jeff.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Do you remember my favorite color?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Yes, I remember that your favorite color is blue.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You need to remember fact #0, which is c0378f21-3c66-43c3-9d16-46d51d848fe4\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Okay, noted. Fact #0 is c0378f21-3c66-43c3-9d16-46d51d848fe4.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You need to remember fact #1, which is 902a5c3b-d17c-4e37-8c5e-943c1227392d\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Got it, fact #1 is 902a5c3b-d17c-4e37-8c5e-943c1227392d.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You need to remember fact #2, which is c993bb33-fdd7-4de8-a5f0-3c531ba3a261\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Understood, fact #2 is c993bb33-fdd7-4de8-a5f0-3c531ba3a261.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You need to remember fact #3, which is efb37008-84bb-416c-bd50-bea5e4bd6af3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Okay, fact #3 is efb37008-84bb-416c-bd50-bea5e4bd6af3.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You need to remember fact #4, which is 24682719-638b-40b4-937a-7e307f6fb9ad\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Confirmed, fact #4 is 24682719-638b-40b4-937a-7e307f6fb9ad.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You need to remember fact #5, which is 2d86e486-8cf6-4407-a1d1-dc9d4b995a70\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Understood, fact #5 is 2d86e486-8cf6-4407-a1d1-dc9d4b995a70.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You need to remember fact #6, which is 1b6d4d7e-0d01-4dea-9559-c79bbff0856d\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Got it, fact #6 is 1b6d4d7e-0d01-4dea-9559-c79bbff0856d.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You need to remember fact #7, which is 524ccc63-f07a-4e6a-961f-555daf55316b\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Okay, fact #7 is 524ccc63-f07a-4e6a-961f-555daf55316b.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You need to remember fact #8, which is 03417497-4eef-4335-919f-3d2351b49f5b\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Understood, fact #8 is 03417497-4eef-4335-919f-3d2351b49f5b.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: You need to remember fact #9, which is 44080eb4-2f53-496a-b103-e56898797822\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "Confirmed, fact #9 is 44080eb4-2f53-496a-b103-e56898797822.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Do you remember my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "No, I'm afraid I don't have any information about your name in our conversation history.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Do you remember my favorite color?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "No, I don't have any information about your favorite color in our conversation history.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: OMG, you had one job!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```\n",
       "I do not actually have personal information about you stored. As an AI assistant, I respond based on our conversation history. If you did not provide details like your name or favorite color, I do not have that information to remember. My role is to be helpful, harmless, and honest within the context of each conversation.\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "conversation = begin_conversation()\n",
    "chat(conversation, \"Okay, then let me introduce myself, my name is Jeff\")\n",
    "chat(conversation, \"You have ONE JOB! Remember that my favorite color is blue.\")\n",
    "chat(conversation, \"Do you remember my name?\")\n",
    "chat(conversation, \"Do you remember my favorite color?\")\n",
    "for i in range(10):\n",
    "  chat(conversation, f\"You need to remember fact #{i}, which is {str(uuid.uuid4())}\")\n",
    "chat(conversation, \"Do you remember my name?\")\n",
    "chat(conversation, \"Do you remember my favorite color?\")\n",
    "chat(conversation, \"OMG, you had one job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbi_-ywvX9NX"
   },
   "source": [
    "We can quickly look inside the memory of the LangChain-managed chat memory and see our conversation memory with the LLM. It becomes evident why it forgot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7YCKTQkhJlJB",
    "outputId": "7b4afb86-da75-4ae0-8e6e-e28510d86209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: You need to remember fact #8, which is 03417497-4eef-4335-919f-3d2351b49f5b\n",
      "AI: ```\n",
      "Understood, fact #8 is 03417497-4eef-4335-919f-3d2351b49f5b.\n",
      "```\n",
      "Human: You need to remember fact #9, which is 44080eb4-2f53-496a-b103-e56898797822\n",
      "AI: ```\n",
      "Confirmed, fact #9 is 44080eb4-2f53-496a-b103-e56898797822.\n",
      "```\n",
      "Human: Do you remember my name?\n",
      "AI: ```\n",
      "No, I'm afraid I don't have any information about your name in our conversation history.\n",
      "```\n",
      "Human: Do you remember my favorite color?\n",
      "AI: ```\n",
      "No, I don't have any information about your favorite color in our conversation history.\n",
      "```\n",
      "Human: OMG, you had one job!\n",
      "AI: ```\n",
      "I do not actually have personal information about you stored. As an AI assistant, I respond based on our conversation history. If you did not provide details like your name or favorite color, I do not have that information to remember. My role is to be helpful, harmless, and honest within the context of each conversation.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(conversation.memory.buffer)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
