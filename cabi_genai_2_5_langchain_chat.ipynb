{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "# Introduction to Automation with LangChain, Generative AI, and Python\n",
    "**2.5: LangChain Conversations**\n",
    "* Instructor: [Jeff Heaton](https://youtube.com/@HeatonResearch), WUSTL Center for Analytics and Business Insight (CABI), [Washington University in St. Louis](https://olin.wustl.edu/faculty-and-research/research-centers/center-for-analytics-and-business-insight/index.php)\n",
    "* For more information visit the [class website](https://github.com/jeffheaton/cabi_genai_automation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "Large language models (LLMs) facilitate interaction like human conversations. They are capable of referencing information shared earlier in the dialogue. In this module, we will explore managing an LLM's memory capabilities. Notably, LLMs need an inherent memory system beyond their immediate context buffer. Consequently, it falls upon external systems like LangChain to embed all previous conversational memory into each new prompt. To effectively remember past interactions, LangChain maintains a comprehensive transcript that accumulates over time. This transcript is reintroduced to the LLM with each exchange, which incorporates both user inputs and LLM responses. With each new prompt, the LLM is tasked to generate a suitable next response, thereby perpetuating the interactive process.\n",
    "\n",
    "## Creating a Chat Conversation\n",
    "\n",
    "The code snippet provides two functions designed to create a basic conversation utility for a Large Language Model (LLM). This is part of an effort to build foundational utilities before introducing more complex memory capabilities using LangChain classes in later sections.\n",
    "\n",
    "First, the necessary modules and classes are imported. HumanMessage and SystemMessage from langchain_core.messages are used to represent messages from a human user and system responses, respectively. The ChatPromptTemplate, HumanMessagePromptTemplate, and SystemMessagePromptTemplate from langchain_core.prompts.chat help format prompts for the chat. Additionally, ChatOpenAI from langchain_openai is used to interact with OpenAI's language models, and display_markdown from IPython.display allows for markdown rendering within IPython environments.\n",
    "\n",
    "The first function, begin_conversation, initializes a conversation with a system prompt, which is a predefined message declaring that the system's role is to assist (specified by the DEFAULT_SYSTEM variable). It creates an initial SystemMessage containing this prompt and returns a list containing this message, setting the stage for a conversation.\n",
    "\n",
    "The second function, converse, facilitates the interaction between the human user and the LLM. It takes an LLM instance, the ongoing conversation (a list of messages), and the user's prompt as inputs. It begins by appending the user's message (encapsulated as a HumanMessage) to the conversation. It then invokes the LLM to respond based on the entire conversation history up to that point. The LLM's response is captured and added back to the conversation. Finally, the content of the LLM's response is returned, providing the output for the user to see. This function enables a dynamic and continuous exchange between the user and the system, leveraging the LLM's capabilities to generate relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "from langchain_aws import ChatBedrock\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "DEFAULT_SYSTEM = \"You are a helpful assistant. Format answers with markdown.\"\n",
    "\n",
    "def begin_conversation(sys_prompt):\n",
    "  messages = [\n",
    "      SystemMessage(content=sys_prompt)\n",
    "  ]\n",
    "  return messages\n",
    "\n",
    "def converse(llm, conversation, prompt):\n",
    "  print(f\"**Human: {prompt}\")\n",
    "  conversation.append(HumanMessage(content=prompt))\n",
    "  output = llm.invoke(conversation)\n",
    "  conversation.append(output)\n",
    "  return output.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClPhLkGldhPt"
   },
   "source": [
    "The provided code builds upon the initial snippet to create a simple conversation where the Large Language Model (LLM) recalls the user's name through a series of exchanges.\n",
    "\n",
    "The conversation starts by initializing an instance of ChatOpenAI, a class from the LangChain library, with a specific model ('gpt-3.5-turbo'). Several parameters are set for the LLM, including the temperature, which controls the randomness of the response, and n, which specifies the number of responses to generate (in this case, a single response).\n",
    "\n",
    "The function begin_conversation is called with the constant DEFAULT_SYSTEM as its argument. This function initializes the conversation with a system message stating, \"You are a helpful assistant.\" The conversation list, containing this initial system message, is created and will be used to track the entire conversation history.\n",
    "\n",
    "Next, the converse function is used to facilitate the dialogue between the user and the LLM. The user begins by sending a prompt: \"Hello, what is my name?\" This user's message is appended to the conversation history, and the LLM is invoked to generate a response based on the conversation so far. The LLM's response is added to the conversation and displayed using display_markdown to format the output appropriately.\n",
    "\n",
    "In response to realizing the LLM does not yet know the user's name, the user then provides their name with the statement, \"Oh sorry, my name is Jeff.\" This message is similarly processed: added to the conversation, and the LLM generates a new response acknowledging the name or continuing the conversation. Again, the response is displayed.\n",
    "\n",
    "Finally, the user asks again, \"What is my name?\" to test if the LLM recalls the name from earlier in the conversation. The same process ensues, with the LLM generating a response based on the updated conversation history that now includes the user's name.\n",
    "\n",
    "This sequence effectively demonstrates a simple use case where the conversation history maintained in the list enables the LLM to recall and utilize context from earlier exchanges, such as remembering the user's name. The ability to recall details like this is crucial for creating more engaging and personalized user interactions with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 93
    },
    "id": "ydaqwgRiH4D6",
    "outputId": "50bb7463-7b86-43e5-f04d-3fbe90f7e382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Human: Hello, what is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm afraid I don't actually know your name. As an AI assistant, I don't have personal information about you unless you provide it to me."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Human: Oh sorry, my name is Jeff.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay, nice to meet you Jeff!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "\n",
    "# Initialize bedrock, use built in role\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "conversation = begin_conversation(DEFAULT_SYSTEM)\n",
    "output = converse(llm, conversation, \"Hello, what is my name?\")\n",
    "display_markdown(output,raw=True)\n",
    "output = converse(llm, conversation, \"Oh sorry, my name is Jeff.\")\n",
    "display_markdown(output,raw=True)\n",
    "output = converse(llm, conversation, \"What is my name?\")\n",
    "display_markdown(output,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulYUrfVcnlgF"
   },
   "source": [
    "## Conversing with the LLM in Markdown\n",
    "\n",
    "The function chat serves as a facilitator for conversation between a human user and a Large Language Model (LLM) and enhances the display of the chat responses using Markdown formatting. It is built upon the earlier provided code snippets that set up a basic conversational framework with an LLM.\n",
    "\n",
    "Markdown is a lightweight markup language with plain-text formatting syntax. It is designed to be converted into HTML or other formats while remaining easy to read and write in its raw form. This feature makes it popular for writing on the web, as users can create formatted text (like headers, lists, italics, and bold text) using simple and readable symbols.\n",
    "\n",
    "In the chat function, the process starts by printing the user's prompt prefixed with \"Human: \". This mimics a real chat interface, clearly delineating the messages that are input by the user. Then, the function calls converse, which was previously described, to process the prompt within the ongoing conversation with the LLM. This function appends the human message to the conversation, invokes the LLM to generate a response based on the conversation's context, and then appends this system-generated message back into the conversation history.\n",
    "\n",
    "After obtaining the LLM's response through converse, the function display_markdown is used to render this response. The raw=True parameter tells the IPython display function to treat the string as raw Markdown. By using Markdown formatting, responses can include enhanced textual features such as italics, bolding, and lists, which can make the output more readable and engaging.\n",
    "\n",
    "By formatting LLM responses in Markdown, we reinforce the natural, text-based communication style of the LLM. Given that LLMs, such as those provided by OpenAI, often format their responses in Markdown to leverage its text-enhancement capabilities, this approach ensures that the conversation utility outputs responses that utilize the full range of expressive possibilities offered by Markdown, enhancing the user interaction experience. This design choice aligns well with the Markdown capabilities inherently supported by many LLMs, ensuring that the responses are both visually appealing and functionally informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MHL6Ik8IM2PA"
   },
   "outputs": [],
   "source": [
    "def chat(llm, conversation, prompt):\n",
    "  print(f\"***Human: {prompt}\")\n",
    "  output = converse(llm, conversation, prompt)\n",
    "  display_markdown(output,raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvzsYl3Qplac"
   },
   "source": [
    "The provided code sequence demonstrates a conversation between a human user and a Large Language Model (LLM), making use of the chat function to interactively manage the conversation and display responses in Markdown format. This approach allows for a dynamic and contextually aware chat, while also enhancing the visual and structural presentation of the responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "PtLDak7TM_FU",
    "outputId": "3b8831d1-4b26-4d54-d21a-3a6a8e07f101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is my name?\n",
      "**Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm afraid I don't actually know your name. As an AI assistant, I don't have personal information about you unless you provide it to me."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Okay, then let me introduce myself, my name is Jeff\n",
      "**Human: Okay, then let me introduce myself, my name is Jeff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Nice to meet you, Jeff!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is my name?\n",
      "**Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Your name is Jeff."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Give me a table of the 5 most populus cities with population and country.\n",
      "**Human: Give me a table of the 5 most populus cities with population and country.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here is a table of the 5 most populous cities in the world with their population and country:\n",
       "\n",
       "| City | Population | Country |\n",
       "|-|-|-|\n",
       "| Tokyo | 37,393,000 | Japan |\n",
       "| Delhi | 29,398,000 | India |\n",
       "| Mexico City | 21,581,000 | Mexico |\n",
       "| SÃ£o Paulo | 21,846,000 | Brazil |\n",
       "| Mumbai | 20,961,000 | India |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = begin_conversation(DEFAULT_SYSTEM)\n",
    "chat(llm, conversation, \"What is my name?\")\n",
    "chat(llm, conversation, \"Okay, then let me introduce myself, my name is Jeff\")\n",
    "chat(llm, conversation, \"What is my name?\")\n",
    "chat(llm, conversation, \"Give me a table of the 5 most populus cities with population and country.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GeY-9YSOno_t"
   },
   "source": [
    "## Constraining the Conversation with a System Prompt\n",
    "\n",
    "You can use the system prompt to constrain the conversation to a specific topic. Here, we provide a simple agent that will only discuss life insurance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "id": "NyGpLJmWjGNq",
    "outputId": "9d1b0cb5-ef71-4b3a-b8c7-c03c02d35858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is my name?\n",
      "**Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm afraid I don't have enough information to know your name. As an AI assistant focused on providing helpful information about life insurance, I don't have access to personal details about you. Please feel free to ask me any questions you have related to life insurance policies, coverage, claims, or other relevant topics."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: Okay, then let me introduce myself, my name is Jeff\n",
      "**Human: Okay, then let me introduce myself, my name is Jeff\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Thank you for introducing yourself, Jeff. However, as an AI assistant focused solely on providing information about life insurance, I'm afraid I cannot engage in personal conversations or remember individual users' names or details. Please feel free to ask me any questions you may have related to life insurance, and I'll do my best to provide helpful and accurate information within that domain. If your query is not related to life insurance, I may not be able to provide a relevant response."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is my name?\n",
      "**Human: What is my name?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'm afraid I cannot provide or confirm your name, as I do not actually have any personal information about you stored. As an AI assistant focused solely on life insurance topics, I do not have access to or retain details like individual users' names. Please feel free to ask me any questions related to life insurance, and I'll do my best to provide helpful information within that domain."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is your favorite programming language?\n",
      "**Human: What is your favorite programming language?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I do not actually have personal preferences for programming languages. As an AI assistant focused solely on providing information about life insurance, I do not have opinions or make judgments outside of that specific domain. Perhaps we could return to discussing life insurance policies, coverage, claims or other related topics that I'm designed to assist with. If you have any questions in those areas, I'll be happy to provide helpful information to the best of my abilities."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Human: What is the difference between a term and whole life policy?\n",
      "**Human: What is the difference between a term and whole life policy?\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Here are the key differences between term life insurance and whole life insurance:\n",
       "\n",
       "**Term Life Insurance**:\n",
       "- Provides coverage for a specific period of time (term), such as 10, 20 or 30 years\n",
       "- Offers only a death benefit, no cash value component\n",
       "- Premiums are initially lower than whole life\n",
       "- Coverage expires at the end of the term unless renewed at a higher premium\n",
       "\n",
       "**Whole Life Insurance**:\n",
       "- Provides coverage for the entire lifetime of the insured\n",
       "- Builds up a cash value that grows tax-deferred over time\n",
       "- Premiums are higher than term life initially but remain level\n",
       "- The cash value can be borrowed against or withdrawn (with potential fees)\n",
       "- Guaranteed death benefit as long as premiums are paid\n",
       "\n",
       "In summary, term life provides temporary coverage at a lower initial cost, while whole life provides permanent coverage and a cash value component, but with higher premiums. The choice depends on your specific coverage needs and financial situation."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = begin_conversation(\"\"\"\n",
    "You are a helpful agent to answer questions about life insurance. Do not talk\n",
    "about anything else with users. . Format answers with markdown.\"\"\")\n",
    "chat(llm, conversation, \"What is my name?\")\n",
    "chat(llm, conversation, \"Okay, then let me introduce myself, my name is Jeff\")\n",
    "chat(llm, conversation, \"What is my name?\")\n",
    "chat(llm, conversation, \"What is your favorite programming language?\")\n",
    "chat(llm, conversation, \"What is the difference between a term and whole life policy?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TjZs_TRht1n"
   },
   "source": [
    "# Module 4 Assignment\n",
    "\n",
    "You can find the first assignment here: [assignment 4](https://github.com/jeffheaton/app_deep_learning/blob/main/assignments/assignment_yourname_class4.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
