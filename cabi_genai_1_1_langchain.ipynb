{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# Introduction to Automation with LangChain, Generative AI, and Python\n",
    "**1.1: Langchain**\n",
    "* Instructor: [Jeff Heaton](https://youtube.com/@HeatonResearch), WUSTL Center for Analytics and Business Insight (CABI), [Washington University in St. Louis](https://olin.wustl.edu/faculty-and-research/research-centers/center-for-analytics-and-business-insight/index.php)\n",
    "* For more information visit the [class website](https://github.com/jeffheaton/cabi_genai_automation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "# 1.1: Introduction to LangChain\n",
    "\n",
    "One of the most intriguing and promising developments in the evolving landscape of language models and artificial intelligence is LangChain. This technology represents a significant leap forward in how we interact with and harness the capabilities of large language models (LLMs). As we delve into the intricacies of LangChain in this chapter, it's important to understand not just the technical underpinnings but also the user experience that makes it so revolutionary.\n",
    "\n",
    "## LangChain Chat Conversation Format\n",
    "\n",
    "To explore LangChain comprehensively, we will adopt a format that has become increasingly familiar and effective in LLMs: the chat conversation interface. This interactive style, reminiscent of how many of us communicate daily, offers a unique and accessible means to illustrate LangChain's capabilities, potential applications, and the nuances of its operation.\n",
    "\n",
    "We begin by importing the components from the LangChain library to support a chat-style interface to Bedrock. We will use the ChatBedrock interface for the Bedrock family of LLM models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K2lC_JK3guaj"
   },
   "outputs": [],
   "source": [
    "# Conversation Style Inteface\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YmWHTAub0sTm"
   },
   "source": [
    "The conversation format consists of arrays of chat entries of the following three types:\n",
    "\n",
    "* **SystemMessage** - This class designates the system prompt that provides instructions to the AI on the nature of the conversation and hints and guidelines. Generally, there will be only one system message at the beginning of the array.\n",
    "* **HumanMessage** - This class designates the chat messages from outside the LLM, typically the human user.\n",
    "* **AIMessage** - This class designates the chat messages from the LLM as responses to the HumanMessage messages.\n",
    "\n",
    "Here we see the chain to ask a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "V_sJoVYAtE6b"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(\n",
    "        content=\"You are a helpful assistant that concisely and accurately.\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is the capital of France?\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To7rZtUO4B8t"
   },
   "source": [
    "We now submit these messages and retrieve the output from the model. We will use the older and less expensive Titan model, which is good enough for this query. Further, we use a zero temperature; we are simply looking for a factual answer, and creativity is not a goal or concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xfb875JhtI6J",
    "outputId": "f4e120de-7a83-4b82-e10c-d8c3f990322b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      " The capital of France is Paris.\n",
      "-----------\n",
      "{'model_id': 'amazon.titan-text-lite-v1', 'usage': {'prompt_tokens': 26, 'completion_tokens': 7, 'total_tokens': 33}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "MODEL = 'amazon.titan-text-lite-v1'\n",
    "#MODEL = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "#MODEL = 'mistral.mistral-7b-instruct-v0:2'\n",
    "#MODEL = 'meta.llama2-70b-chat-v1'\n",
    "\n",
    "# Initialize bedrock, use built in role\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "print(\"Model response:\")\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)\n",
    "print(\"-----------\")\n",
    "print(output.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be6a0hXj4lHj"
   },
   "source": [
    "The model that LangChain returns to you returns additional metadata. This data shows the token usage, which might be useful for estimating the total cost expected from this query.\n",
    "\n",
    "We can continue to grow this conversation if we wish. To do so, we added the model's response and another human question. Here, we will ask the model if it was sure about its last response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2vJuMVHZ1jJ8",
    "outputId": "d34c6945-9d2b-41c4-8e02-33b5056a72d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemMessage : You are a helpful assistant that concisely and accurately.\n",
      "HumanMessage : What is the capital of France?\n",
      "AIMessage :  The capital of France is Paris.\n",
      "HumanMessage : Are you sure, I think it was renamed for some reason?\n"
     ]
    }
   ],
   "source": [
    "messages.append(output)\n",
    "messages.append(HumanMessage(content=\"Are you sure, I think it was renamed for some reason?\"))\n",
    "for message in messages:\n",
    "    print(f\"{type(message).__name__} : {message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8Z7DZRH5TFr"
   },
   "source": [
    "We can submit the conversation array to the model and see its latest response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9qFxkEmAz1w1",
    "outputId": "f1928a10-db7f-4abc-92af-23e24cd8ecb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n",
      " Paris is the capital of France. It has been the capital since 1792.\n"
     ]
    }
   ],
   "source": [
    "print(\"Model response:\")\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9nvX9oA3a0Z"
   },
   "source": [
    "## Asking a Single Question\n",
    "\n",
    "If you wish to ask the model a single question, not as part of a conversation chain, you can pass a string to the model for a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "34-95k0qg8ss",
    "outputId": "5d93f5a9-1ba6-4b52-d3ec-a4a8fa76451f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The five largest cities in the USA by population are: New York City, Los Angeles, Chicago, Houston, and Phoenix.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the question\n",
    "question = \"What are the five largest cities in the USA by population?\"\n",
    "\n",
    "# Use Langchain to call the API\n",
    "# The method and parameters might differ based on the Langchain version\n",
    "response = llm.invoke(question)\n",
    "\n",
    "# Print the response\n",
    "display(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o2EkUkN3hEX"
   },
   "source": [
    "## Prompt Templates\n",
    "\n",
    "LangChain allows you to create chains of operations typically performed as part of an LLM-enabled application. One of these operations is a prompt template, which allows you to insert text into a previously created prompt. In this example, we will create a prompt template that asks the model to create a random blog post title.\n",
    "\n",
    "```\n",
    "Return only the title of a blog post article title on the topic of {topic} in {language}\n",
    "```\n",
    "\n",
    "To accomplish this objective, we will use a **PromptTemplate** object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L-g3zOXBmulc",
    "outputId": "fb7a08ec-2f47-4526-9605-c0116289334f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mReturn only the title of a blog post article title on the topic of pets for data scientists in english\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'topic': 'pets for data scientists', 'language': 'english', 'text': ' Hello! Here is the title of a blog post article on the topic of pets for data scientists:\\n\\n\"Scikit-Learn: Retrieving Image Features from Pet Blogs Using Natural Language Processing\"'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "\n",
    "# Higher temperature, more creative\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL,\n",
    "    model_kwargs={\"temperature\": 0.9},\n",
    ")\n",
    "\n",
    "topic = \"pets for data scientists\"\n",
    "language = \"english\"\n",
    "\n",
    "title_template = PromptTemplate( input_variables = ['topic', 'language'],\\\n",
    "  template = 'Return only the title of a blog post article title on the topic of {topic} in {language}' )\n",
    "title_chain = LLMChain(llm=llm, prompt=title_template, verbose=True)\n",
    "response = title_chain.invoke({'topic':topic,'language':language })\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IOpAcj-byBPg"
   },
   "source": [
    "## Create a Simple Sequential Chain\n",
    "\n",
    "We will now use LangChain to tie multiple LLM calls into a longer chain using the **SimpleSequentialChain** class. We will use two smaller chains to create a title and body text for a blog post. We begin by defining the two prompts we will use to construct this blog post. Also, note that we request that the LLM utilize [markdown](https://en.wikipedia.org/wiki/Markdown) to generate the actual blog post.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ELS-9JD3Sao6"
   },
   "outputs": [],
   "source": [
    "# Create the two prompt templates\n",
    "title_template = PromptTemplate( input_variables = ['topic'], template = 'Give me a blog post title on {topic} in English' )\n",
    "article_template = PromptTemplate( input_variables = ['title'], template = 'Write a blog post for {title}, format in markdown.' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3vy7jf3-_xl"
   },
   "source": [
    "We will create the first chain to generate the random title. Here, we allow the user to specify the topic. We use a higher temperature to increase the creativity of the title. We also use a simpler model to minimize cost for the relatively simple task of title selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "_KV8UJUDyP8M"
   },
   "outputs": [],
   "source": [
    "MODEL = 'amazon.titan-text-lite-v1'\n",
    "\n",
    "# Higher temperature, more creative\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL,\n",
    "    model_kwargs={\"temperature\": 0.9},\n",
    ")\n",
    "title_chain = LLMChain(llm=llm, prompt=title_template, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO0ielMf_xRa"
   },
   "source": [
    "Next, we compose the actual blog post; we will use a lower temperature to decrease creativity and cause the LLM to stick to factual information and avoid hallucinations. We also use a more complex model to provide a better article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "G7OCQCggyHlB"
   },
   "outputs": [],
   "source": [
    "MODEL2 = 'meta.llama2-70b-chat-v1'\n",
    "\n",
    "# Create the article chain, stronger mode, still creative\n",
    "llm2 = ChatBedrock(\n",
    "    model_id=MODEL2,\n",
    "    model_kwargs={\"temperature\": 0.9},\n",
    ")\n",
    "article_chain = LLMChain(llm=llm2, prompt=article_template, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rftFvMyBa4-"
   },
   "source": [
    "Now, we combine these two chains into one. The input to the first chain will be the selected topic. The first chain will then output the title to the second chain, which will, in turn, output the actual article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "2ibHBt4eyKZt"
   },
   "outputs": [],
   "source": [
    "# Create a complete chain to create a new blog post\n",
    "complete_chain=SimpleSequentialChain(chains=[title_chain, article_chain], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_7P-n_DBvpD"
   },
   "source": [
    "We can now display the final article. In this case, we requested an article on \"photography,\" and displayed the final article's markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mwro_kfKXvml",
    "outputId": "d0118b4d-9c33-4882-8d5a-0fe4eb42ad59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGive me a blog post title on photography in English\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m Sure, here are some blog post title ideas related to photography:\n",
      "\n",
      "1. How to Take a Professional-Looking Photo\n",
      "2. The Art of Photography: Capturing the Perfect Shot\n",
      "3. Tips for Taking Better Travel Photos\n",
      "4. Street Photography: Capturing the Essence of Everyday Life\n",
      "5. Learn How to Take Better Photos with These Simple Techniques\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a blog post for  Sure, here are some blog post title ideas related to photography:\n",
      "\n",
      "1. How to Take a Professional-Looking Photo\n",
      "2. The Art of Photography: Capturing the Perfect Shot\n",
      "3. Tips for Taking Better Travel Photos\n",
      "4. Street Photography: Capturing the Essence of Everyday Life\n",
      "5. Learn How to Take Better Photos with These Simple Techniques, format in markdown.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m  Sure, here's a blog post on \"Tips for Taking Better Travel Photos\" in markdown format:\n",
      "\n",
      "Tips for Taking Better Travel Photos\n",
      "====================================\n",
      "\n",
      "When it comes to traveling, capturing the memories of your trip is just as important as the experience itself. However, taking great travel photos can be a challenge, especially if you're not a professional photographer. But don't worry, with a few simple tips and tricks, you can take better travel photos that will make your friends and family jealous.\n",
      "\n",
      "Understand Your Camera\n",
      "--------------------\n",
      "\n",
      "The first step to taking better travel photos is to understand your camera. Whether you're using a DSLR, mirrorless, or a smartphone, it's important to know the basics of how it works. Learn about aperture, shutter speed, and ISO, and how to adjust them to suit your needs.\n",
      "\n",
      "Composition is Key\n",
      "-----------------\n",
      "\n",
      "Composition is the key to taking great photos. When taking travel photos, look for interesting subjects such as landmarks, people, and landscapes. Use the rule of thirds to position your subject and create visually appealing photos. Experiment with different angles and perspectives to add depth and interest to your photos.\n",
      "\n",
      "Make the Most of Lighting\n",
      "----------------------\n",
      "\n",
      "Lighting is one of the most important factors in photography. When taking travel photos, make the most of natural light by shooting during the golden hour, which is usually an hour before sunset or after sunrise. Avoid shooting during midday when the sun is too harsh, and use shadows to create interesting effects.\n",
      "\n",
      "Tell a Story\n",
      "------------\n",
      "\n",
      "Travel photos are not just about capturing a moment, but also about telling a story. Try to capture photos that showcase the local culture, people, and way of life. Take photos of street vendors, local markets, and traditional dances to give your audience a glimpse into the place you're visiting.\n",
      "\n",
      "Experiment with Different Techniques\n",
      "----------------------------------\n",
      "\n",
      "Don't be afraid to experiment with different techniques to take your travel photos to the next level. Try using a slow shutter speed to create a sense of movement, or play with depth of field to create a bokeh effect. Use a wide-angle l\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display_markdown\n",
    "\n",
    "article = complete_chain.invoke('photography')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1pN8CbXCE6F"
   },
   "source": [
    "The actual display of the markdown is handled by this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 677
    },
    "id": "Vt5uGJawuru_",
    "outputId": "7ff949a4-294f-43fb-afb7-7785aa22b673"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "  Sure, here's a blog post on \"Tips for Taking Better Travel Photos\" in markdown format:\n",
       "\n",
       "Tips for Taking Better Travel Photos\n",
       "====================================\n",
       "\n",
       "When it comes to traveling, capturing the memories of your trip is just as important as the experience itself. However, taking great travel photos can be a challenge, especially if you're not a professional photographer. But don't worry, with a few simple tips and tricks, you can take better travel photos that will make your friends and family jealous.\n",
       "\n",
       "Understand Your Camera\n",
       "--------------------\n",
       "\n",
       "The first step to taking better travel photos is to understand your camera. Whether you're using a DSLR, mirrorless, or a smartphone, it's important to know the basics of how it works. Learn about aperture, shutter speed, and ISO, and how to adjust them to suit your needs.\n",
       "\n",
       "Composition is Key\n",
       "-----------------\n",
       "\n",
       "Composition is the key to taking great photos. When taking travel photos, look for interesting subjects such as landmarks, people, and landscapes. Use the rule of thirds to position your subject and create visually appealing photos. Experiment with different angles and perspectives to add depth and interest to your photos.\n",
       "\n",
       "Make the Most of Lighting\n",
       "----------------------\n",
       "\n",
       "Lighting is one of the most important factors in photography. When taking travel photos, make the most of natural light by shooting during the golden hour, which is usually an hour before sunset or after sunrise. Avoid shooting during midday when the sun is too harsh, and use shadows to create interesting effects.\n",
       "\n",
       "Tell a Story\n",
       "------------\n",
       "\n",
       "Travel photos are not just about capturing a moment, but also about telling a story. Try to capture photos that showcase the local culture, people, and way of life. Take photos of street vendors, local markets, and traditional dances to give your audience a glimpse into the place you're visiting.\n",
       "\n",
       "Experiment with Different Techniques\n",
       "----------------------------------\n",
       "\n",
       "Don't be afraid to experiment with different techniques to take your travel photos to the next level. Try using a slow shutter speed to create a sense of movement, or play with depth of field to create a bokeh effect. Use a wide-angle l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_key = complete_chain.output_key\n",
    "\n",
    "display_markdown(article[output_key], raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
