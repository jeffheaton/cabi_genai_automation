{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "# Introduction to Automation with LangChain, Generative AI, and Python\n",
    "**3.1: Other Parsers (CSV, JSON, Pandas, Datetime)**\n",
    "* Instructor: [Jeff Heaton](https://youtube.com/@HeatonResearch), WUSTL Center for Analytics and Business Insight (CABI), [Washington University in St. Louis](https://olin.wustl.edu/faculty-and-research/research-centers/center-for-analytics-and-business-insight/index.php)\n",
    "* For more information visit the [class website](https://github.com/jeffheaton/cabi_genai_automation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "In this section, we'll explore how LangChain offers versatile parsers capable of handling a variety of data formats, enhancing its functionality across numerous applications. Among these, it can seamlessly integrate with data in the form of Pandas dataframes, comma-separated lists, JSON structures, and datetime objects, among others. This capability ensures that LangChain can adapt to diverse data inputs, making it a powerful tool for data manipulation and analysis in different contexts. We will delve into some of these parsers and demonstrate their practical applications, highlighting how they can be utilized to streamline processes and extract meaningful insights from data.\n",
    "\n",
    "## Parse Comma Separated List Response\n",
    "\n",
    "We will begin with the CommaSeparatedListOutputParser parser, which can take the LLM output in a comma-separated list and extract it as a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-Vds7BAWITTH"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"List ten {subject}.\\n{format_instructions}\",\n",
    "    input_variables=[\"subject\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "MODEL = 'mistral.mistral-7b-instruct-v0:2'\n",
    "# Initialize bedrock, use built in role\n",
    "model = ChatBedrock(\n",
    "    model_id=MODEL,\n",
    "    model_kwargs={\"temperature\": 0.1},\n",
    ")\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs9tTrVh78Sy"
   },
   "source": [
    "Extract a list of cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W7gvIB0kxORP",
    "outputId": "f6e4b2c4-0153-4e77-b9df-263f48b32b9a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York',\n",
       " 'London',\n",
       " 'Paris',\n",
       " 'Tokyo',\n",
       " 'Sydney',\n",
       " 'Beijing',\n",
       " 'Mumbai',\n",
       " 'Istanbul',\n",
       " 'Seoul',\n",
       " 'Rio de Janeiro']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"cities\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z-cMptW7-wq"
   },
   "source": [
    "Extract a list of programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ljZKh6PKxOIi",
    "outputId": "ab7548ff-3013-42df-9eda-f84b378be275"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are ten programming languages',\n",
       " 'listed as comma-separated values:\\n\\nJava',\n",
       " 'Python',\n",
       " 'C++',\n",
       " 'JavaScript',\n",
       " 'Ruby',\n",
       " 'Swift',\n",
       " 'PHP',\n",
       " 'Kotlin',\n",
       " 'Go',\n",
       " 'R.\\n\\nThese languages are popular and widely used in various industries and applications.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"subject\": \"programming languages\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE2avWGaysc3"
   },
   "source": [
    "## Parse JSON Response\n",
    "\n",
    "We can format the output from the LLM into JSON. For this example, we will accept a sentence that we detect as English and then translate it into Spanish, French, and Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8pvbqwz9xOAQ",
    "outputId": "603e3fd8-6991-444d-c88c-e48438b8a04b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'detected': 'English',\n",
       " 'spanish': 'No se comprende tu consulta',\n",
       " 'french': 'Je ne comprends pas ta requête',\n",
       " 'chinese': '您的查询我理解不了'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Define your desired data structure.\n",
    "class Translate(BaseModel):\n",
    "  detected: str = Field(description=\"the detected language of the input\")\n",
    "  spanish: str = Field(description=\"the input translated to Spanish\")\n",
    "  french: str = Field(description=\"the input translated to French\")\n",
    "  chinese: str = Field(description=\"the input translated to Chinese\")\n",
    "\n",
    "# And a query intented to prompt a language model to populate the data structure.\n",
    "input_text = \"What is your name?\"\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "parser = JsonOutputParser(pydantic_object=Translate)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{input}\\n\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "chain.invoke({\"input\": input_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAY721mN14kp"
   },
   "source": [
    "## Query Pandas Dataframe\n",
    "\n",
    "Langchain's capabilities include parsing and analyzing Pandas dataframes using the PandasDataFrameOutputParser. This feature allows users to seamlessly integrate data stored in Pandas dataframes and use Langchain to query and extract insights from this data. By leveraging the PandasDataFrameOutputParser, Langchain can interpret the dataframe's structure, contents, and context, enabling it to provide accurate answers to user queries. This integration is particularly useful for data analysis, enabling more interactive and natural language-based exploration of data stored in Pandas dataframes.\n",
    "\n",
    "The following code reads and displays the first lines from the classic [iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VMP2U3ZJ0hDP",
    "outputId": "5cc7b4c6-2ad6-4458-ec38-32a9f8095de7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_l  sepal_w  petal_l  petal_w      species\n",
      "0      5.1      3.5      1.4      0.2  Iris-setosa\n",
      "1      4.9      3.0      1.4      0.2  Iris-setosa\n",
      "2      4.7      3.2      1.3      0.2  Iris-setosa\n",
      "3      4.6      3.1      1.5      0.2  Iris-setosa\n",
      "4      5.0      3.6      1.4      0.2  Iris-setosa\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from typing import Any, Dict\n",
    "\n",
    "import pandas as pd\n",
    "from langchain.output_parsers import PandasDataFrameOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Load the iris dataset\n",
    "df = pd.read_csv(\n",
    "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", na_values=[\"NA\", \"?\"]\n",
    ")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6errbnrFA1SC"
   },
   "source": [
    "Next we load the iris dataframe into a PandasDataFrameOutputParser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5h1Y5yhKxNzS"
   },
   "outputs": [],
   "source": [
    "#MODEL = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "#llm = ChatBedrock(\n",
    "#    model_id=MODEL,\n",
    "#    model_kwargs={\"temperature\": 0.1},\n",
    "#)\n",
    "\n",
    "parser = PandasDataFrameOutputParser(dataframe=df)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZhroER0yBG5G"
   },
   "source": [
    "We query for the sum of one of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lE-U8jO24oBY",
    "outputId": "82e8b8d4-4ffc-4597-8b11-de4825923f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sum': 179.90000000000003}\n"
     ]
    }
   ],
   "source": [
    "query = \"Get the sum of petal_w column.\"\n",
    "parser_output = chain.invoke({\"query\": query})\n",
    "print(parser_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y09-epHS5McO"
   },
   "source": [
    "## Datetime\n",
    "\n",
    "Langchain includes a feature known as the DatetimeOutputParser, which is specifically designed to parse datetime values from text. This capability allows it to recognize and interpret dates and times expressed in various formats, converting them into a standardized datetime format. This functionality is invaluable in applications involving scheduling, data analysis, or any context where accurate handling of dates and times is essential. By utilizing the DatetimeOutputParser, developers can streamline the processing of temporal data, ensuring that their applications can effectively manage and respond to time-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "NRuzzhXF5Oa1"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import DatetimeOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "output_parser = DatetimeOutputParser()\n",
    "template = \"\"\"Answer the users question:\n",
    "\n",
    "{question}\n",
    "\n",
    "{format_instructions}\"\"\"\n",
    "prompt = PromptTemplate.from_template(\n",
    "    template,\n",
    "    partial_variables={\"format_instructions\": output_parser.get_format_instructions()},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roqInbqGFdUb"
   },
   "source": [
    "We can display that prompt that we will use to obtain dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HecixPQl6kYk",
    "outputId": "e688b1a8-b675-4793-d8c9-e0ac2fe01142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['question'] partial_variables={'format_instructions': \"Write a datetime string that matches the following pattern: '%Y-%m-%dT%H:%M:%S.%fZ'.\\n\\nExamples: 458-12-04T04:36:16.905956Z, 714-07-04T10:35:44.885803Z, 283-04-16T04:04:00.424015Z\\n\\nReturn ONLY this string, no other words!\"} template='Answer the users question:\\n\\n{question}\\n\\n{format_instructions}'\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kg5ZkOetFiPx"
   },
   "source": [
    "We create the chain that we will use to parse dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2w5gYty_5pKd"
   },
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dD-gNvBHFooj"
   },
   "source": [
    "We will query for two dates, one real and the other fictional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ymu48Ggg518q",
    "outputId": "4aa29516-440c-4cd7-8ee7-ec18a8a4c4bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1994-03-20 04:00:00\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\"question\": \"When was the Python language introduced? Your response should just be a date string, otherwise I can't parse it.\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nOPWYpxr6As1",
    "outputId": "b9529904-92e8-4248-f686-a1208ef6be09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2277-10-23 14:35:12.012345\n"
     ]
    }
   ],
   "source": [
    "output = chain.invoke({\"question\": \"What is the date of the war in the video game Fallout? Your response should just be a single date string, otherwise I can't parse it.\"})\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
