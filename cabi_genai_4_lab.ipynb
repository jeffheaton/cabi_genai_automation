{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "# Introduction to Automation with LangChain, Generative AI, and Python\n",
    "**Lab 4: Add Pictures to Book**\n",
    "* Instructor: [Jeff Heaton](https://youtube.com/@HeatonResearch), WUSTL Center for Analytics and Business Insight (CABI), [Washington University in St. Louis](https://olin.wustl.edu/faculty-and-research/research-centers/center-for-analytics-and-business-insight/index.php)\n",
    "* For more information visit the [class website](https://github.com/jeffheaton/cabi_genai_automation).\n",
    "\n",
    "# Instructions\n",
    "\n",
    "Recall the book generator that we saw earlier? Lets add a cover picture to the title page. Below is a function that will render an image from the specified prompt and output to a filename. This is the same code that was presented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "from PIL import Image\n",
    "import botocore\n",
    "\n",
    "def render_diffusion(prompt, filename, height, width):\n",
    "    boto3_bedrock = boto3.client('bedrock-runtime')\n",
    "    negative_prompts = [\n",
    "        \"poorly rendered\",\n",
    "        \"poor background details\",\n",
    "        \"writing\",\n",
    "        \"text\",\n",
    "    ]\n",
    "    style_preset = \"photographic\"\n",
    "    clip_guidance_preset = \"FAST_GREEN\"\n",
    "    sampler = \"K_DPMPP_2S_ANCESTRAL\"\n",
    "\n",
    "    request = json.dumps({\n",
    "        \"text_prompts\": (\n",
    "            [{\"text\": prompt, \"weight\": 1.0}]\n",
    "            + [{\"text\": negprompt, \"weight\": -1.0} for negprompt in negative_prompts]\n",
    "        ),\n",
    "        \"cfg_scale\": 5,\n",
    "        \"seed\": 42,\n",
    "        \"steps\": 60,\n",
    "        \"style_preset\": style_preset,\n",
    "        \"clip_guidance_preset\": clip_guidance_preset,\n",
    "        \"sampler\": sampler,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    })\n",
    "    modelId = \"stability.stable-diffusion-xl-v1\"\n",
    "\n",
    "    response = boto3_bedrock.invoke_model(body=request, modelId=modelId)\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "    print(response_body[\"result\"])\n",
    "    base_64_img_str = response_body[\"artifacts\"][0].get(\"base64\")\n",
    "    image = Image.open(io.BytesIO(base64.decodebytes(bytes(base_64_img_str, \"utf-8\"))))\n",
    "    image.save(filename)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provided here is the book code from before. Modify this code to add an image that you generate with the render_diffusion function. Claude 3 can give you the code to add an image to the markdown, but it may not be able to make the complete change. You should also call the LLM to transform the synopsis into an image generation prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Collateral Deception\n",
      "Synopsis: A high-stakes international spy thriller, 'Collateral Deception' follows the exploits of CIA operative Sarah Knowles as she unravels a web of deceit and betrayal that threatens global security. After a routine mission goes awry, Sarah finds herself embroiled in a deadly conspiracy involving rogue agents, powerful corporations, and a sinister plot that could ignite a new world war. With the fate of nations hanging in the balance, Sarah must navigate a treacherous landscape of lies and double-crosses, relying on her wits and skills to uncover the truth before it's too late.\n",
      "Rendering chapter 1/13: Prologue: The Calm Before the Storm\n",
      "Rendering chapter 2/13: Whispers in the Night\n",
      "Rendering chapter 3/13: Echoes of Betrayal\n",
      "Rendering chapter 4/13: Shadows in the Boardroom\n",
      "Rendering chapter 5/13: The Rabbit Hole\n",
      "Rendering chapter 6/13: Ghosts of the Past\n",
      "Rendering chapter 7/13: The Moscow Connection\n",
      "Rendering chapter 8/13: Infiltration\n",
      "Rendering chapter 9/13: The Mole's Dilemma\n",
      "Rendering chapter 10/13: Shifting Alliances\n",
      "Rendering chapter 11/13: Checkmate\n",
      "Rendering chapter 12/13: Endgame\n",
      "Rendering chapter 13/13: Epilogue: The Aftermath\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports organized by source and purpose\n",
    "from IPython.display import display_markdown\n",
    "import markdown\n",
    "import pdfkit\n",
    "\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "# Constants used in the program\n",
    "MODEL = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "SUBJECT = \"international spy story\"\n",
    "\n",
    "# Initialize ChatBedrock with specified model and settings\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL,\n",
    "    model_kwargs={\"temperature\": 0.7}\n",
    ")\n",
    "\n",
    "def query_llm(prompt):\n",
    "    \"\"\"Query the language model with a formatted prompt and return the response content.\"\"\"\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=\"\"\"You are writing a book, return just what I ask you for. \n",
    "                       Do not add comments, it is very important that you only \n",
    "                       provide the final output without any additional comments \n",
    "                       or remarks. Do not repeat my command or a summary of my command.\"\"\"\n",
    "        ),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "\n",
    "    output = llm.invoke(messages)\n",
    "    return output.content.strip(\" '\\\"\")\n",
    "\n",
    "# Generate book title, synopsis, and table of contents\n",
    "title = query_llm(f\"Give me a random title for a book on the subject '{SUBJECT}'. Return only the title, no additional text.\")\n",
    "print(f\"Title: {title}\")\n",
    "\n",
    "synopsis = query_llm(f\"Give me just a synopsis for a book of the title '{title}' on the subject '{SUBJECT}'.\")\n",
    "print(f\"Synopsis: {synopsis}\")\n",
    "\n",
    "\n",
    "toc = query_llm(f\"Give me a table of contents for a book of the title '{title}' on the subject '{SUBJECT}' \"\n",
    "                f\"the book synopsis is '{synopsis}'. Return the table of contents as a list of chapter titles. \"\n",
    "                f\"Separate the chapter number and chapter title with a pipe character '|'. \"\n",
    "                f\"Do not give me any line that is not a chapter!\")\n",
    "\n",
    "# Split the table of contents into lines and extract chapter titles\n",
    "lines = toc.splitlines()\n",
    "toc2 = [line.split('|')[1].strip() for line in lines if line]\n",
    "\n",
    "# Render the book chapters\n",
    "book = f\"# {title}\\n{synopsis}\\n\\n## Table of Contents\\n\\n\" + \"\\n\".join(f\"{i+1}. {title}\" for i, title in enumerate(toc2))\n",
    "\n",
    "def render_chapter(num, chapter_title):\n",
    "    \"\"\"Generate content for a chapter given the chapter number and title.\"\"\"\n",
    "    txt = query_llm(f\"Write Chapter {num}, titled '{chapter_title}' for a book of the title '{title}' on \"\n",
    "                    f\"the subject '{SUBJECT}' the book synopsis is '{synopsis}' the table of contents is '{toc}'. \"\n",
    "                    f\"Give me only the chapter text, no chapter heading, no chapter title, number, no additional text. \"\n",
    "                    f\"Make sure the chapter is at least 3,000 to 4,000 words. End with a complete sentence, do not cut off.\")\n",
    "    return txt\n",
    "\n",
    "# Render the entire book\n",
    "for chapter_num, chapter_title in enumerate(toc2, start=1):\n",
    "    print(f\"Rendering chapter {chapter_num}/{len(toc2)}: {chapter_title}\")\n",
    "    chapter_text = render_chapter(chapter_num, chapter_title)\n",
    "    book += f\"\\n\\n## Chapter {chapter_num}: {chapter_title}\\n{chapter_text}\"\n",
    "\n",
    "# Convert Markdown to HTML and then to PDF\n",
    "html = markdown.markdown(book)\n",
    "pdfkit.from_string(html, 'output.pdf', options={\"enable-local-file-access\": \"\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
