{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "# Introduction to Automation with LangChain, Generative AI, and Python\n",
    "**2.3: Text Summarization**\n",
    "* Instructor: [Jeff Heaton](https://youtube.com/@HeatonResearch), WUSTL Center for Analytics and Business Insight (CABI), [Washington University in St. Louis](https://olin.wustl.edu/faculty-and-research/research-centers/center-for-analytics-and-business-insight/index.php)\n",
    "* For more information visit the [class website](https://github.com/jeffheaton/cabi_genai_automation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "Large Language Models (LLMs) like GPT-4 can be utilized to summarize text by extracting key information and presenting it in a concise format. They work by understanding the context and semantic relationships within the original text and then generating a shorter version that retains the essential messages. This process involves natural language understanding and generation capabilities, allowing LLMs to interpret various types of texts, from technical articles to narratives, and produce summaries that are coherent and relevant. The ability to customize the length and focus of the summary based on user preferences makes LLMs particularly effective for digesting large volumes of information quickly and efficiently.\n",
    "\n",
    "## Summarize Single PDF\n",
    "\n",
    "We will begin by seeing how to summarize a single PDF. LangChang loads document types, such as PDFs, using a document loader. There are document loaders for various data types. The following code summarizes a single PDF using a generic summarization system prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dfISNTpwOKiZ"
   },
   "outputs": [],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain import OpenAI, PromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "#MODEL = 'mistral.mistral-7b-instruct-v0:2'\n",
    "#MODEL = 'meta.llama2-70b-chat-v1'\n",
    "MODEL = 'amazon.titan-text-lite-v1'\n",
    "\n",
    "# Initialize bedrock, use built in role\n",
    "llm = ChatBedrock(\n",
    "    model_id=MODEL,\n",
    "    model_kwargs={\"temperature\": 0.2},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLUWbW7NRp4m"
   },
   "source": [
    "\n",
    "The following code snippet demonstrates how to use a specific 'load_summarize_chain' function to set up a summarization process using a Large Language Model (LLM) with a \"map_reduce\" chain type. It starts by loading a PDF from the given URL (\"https://arxiv.org/pdf/1706.03762\") using the 'PyPDFLoader'. The loaded document is then split into manageable parts ('load_and_split'). These parts are fed into the summarization chain ('chain.run(docs)'), which processes and condenses the content. Finally, the summarized content is displayed in markdown format directly within the output environment, ensuring that the formatting of the summary remains intact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 116
    },
    "id": "pDmTQPXgqQfd",
    "outputId": "37708678-596e-4ace-faf5-329f6a75d03b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1417 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       " The Transformer model is a neural network architecture that can generate coherent text from a source text. It has a stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, and a residual connection and layer normalization. The model is capable of generating text in different languages and can handle long input sequences. The paper explores the use of self-attention layers in sequence transduction models, which are used to map one variable-length sequence of symbol representations to another sequence of equal length. The authors find that self-attention layers achieve comparable or better performance on various tasks, while requiring significantly fewer sequential"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "url = \"https://arxiv.org/pdf/1706.03762\"\n",
    "loader = PyPDFLoader(url)\n",
    "docs = loader.load_and_split()\n",
    "summary = chain.invoke(docs)['output_text']\n",
    "display_markdown(summary, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uA1iXHlyMSUU"
   },
   "source": [
    "## Summarize with Custom Prompt\n",
    "\n",
    "LangChain also allows the use of custom system prompts to tailor text summarization according to specific requirements, such as summarizing content in a different language. This flexibility is showcased in the provided code, where a custom prompt template instructs the system to write a concise summary in Spanish. The template is set up to include a placeholder for the text that needs summarizing, followed by an instruction in Spanish to produce a summary. This custom prompt is then incorporated into the summarization process by configuring both the 'map_prompt' and 'combine_prompt' parameters of the 'load_summarize_chain' function. The process begins by downloading a PDF from a specified URL using 'PyPDFLoader', splitting the document into sections, and then applying the summarization chain with the custom prompt to generate a summarized output in Spanish. The summarized content is then displayed in markdown format to maintain proper formatting. This example illustrates the adaptability of LangChain in handling complex summarization tasks that include language-specific instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 133
    },
    "id": "lpQmmSiNMZNY",
    "outputId": "6dbf3e1f-a34c-48cb-fc99-16560ddf9bba"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "The Transformer model is a neural network architecture that can learn to sequence text. It has several layers of neural networks, each with a different purpose. The first layer is a position-wise feed-forward network, which takes the input and maps it to a fixed size. The second layer is a multi-head attention layer, which attaches different parts of the input to different parts of the output. The third layer is a feed-forward network with a softmax activation function, which takes the output of the attention layer and maps it to the output size. The fourth layer is a position-wise feed-forward network, which takes"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TEMPLATE = \"\"\"\n",
    "Write a concise summary of the information presented. Write the summary in Spanish.\n",
    "\n",
    "{text}\n",
    "\n",
    "SUMMARY:\"\"\"\n",
    "PROMPT = PromptTemplate(template=TEMPLATE, input_variables=[\"text\"])\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt=PROMPT, combine_prompt=PROMPT)\n",
    "\n",
    "url = \"https://arxiv.org/pdf/1706.03762\"\n",
    "loader = PyPDFLoader(url)\n",
    "docs = loader.load_and_split()\n",
    "summary = chain.invoke(docs)['output_text']\n",
    "display_markdown(summary, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3MVbKWNsLL3"
   },
   "source": [
    "## Summarize Multiple PDFs\n",
    "\n",
    "We will now see how to summarize multiple documents into one. We will summarize the following four papers, each of which is very important to the field of GenAI.\n",
    "\n",
    "* \"[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)\" by Ashish Vaswani et al. (2017) - This paper introduced the Transformer architecture, which has become the backbone of most modern natural language processing systems, including text-to-text generative models like GPT and BERT.\n",
    "* \"[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)\" by Jacob Devlin et al. (2018) - BERT (Bidirectional Encoder Representations from Transformers) revolutionized the way contextual information is handled by using a bidirectional training of Transformer models. This methodology significantly improved the performance of models on various NLP tasks.\n",
    "* \"[Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)\" by Tom B. Brown et al. (2020) - Also known as the GPT-3 paper, it explores the capabilities of very large transformer-based models, demonstrating that scaling up the size of the models improves performance across a broad spectrum of NLP tasks, often requiring little to no task-specific data.\n",
    "* \"[Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/pdf/1910.10683)\" by Colin Raffel et al. (2019) - This paper introduces T5 (Text-to-Text Transfer Transformer), which converts all NLP tasks into a unified text-to-text format, simplifying the application of transfer learning across different tasks.\n",
    "\n",
    "We use the same process demonstrated to load all these PDF documents and concatenate their summaries into an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIVM9OwssPZm",
    "outputId": "eb0c7a93-68fc-42ff-c5f0-440395757179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: https://arxiv.org/pdf/1706.03762\n",
      "Reading: https://arxiv.org/pdf/1810.04805\n",
      "Reading: https://arxiv.org/pdf/2005.14165\n",
      "Reading: https://arxiv.org/pdf/1910.10683\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "  \"https://arxiv.org/pdf/1706.03762\",\n",
    "  \"https://arxiv.org/pdf/1810.04805\",\n",
    "  \"https://arxiv.org/pdf/2005.14165\",\n",
    "  \"https://arxiv.org/pdf/1910.10683\"\n",
    "]\n",
    "\n",
    "summaries = []\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "for url in urls:\n",
    "  print(f\"Reading: {url}\")\n",
    "  loader = PyPDFLoader(url)\n",
    "  docs = loader.load_and_split()\n",
    "  chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "  summary = chain.invoke(docs)['output_text']\n",
    "  summaries.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QE79kzVUTZ15"
   },
   "source": [
    "After obtaining individual summaries of articles, the next step involves combining these summaries into a single, comprehensive overview. The provided code accomplishes this by first merging all the initial summaries into one long string. To manage the potentially large amount of text, it uses the CharacterTextSplitter class from LangChain to split this combined text into manageable chunks. Each chunk maintains a size of 500 characters with an overlap of 100 characters to ensure continuity and context are preserved across chunks. These chunks are then converted into Document objects, each holding a segment of the summarized text. A new summarization chain is loaded using the same 'map_reduce' model to process these document objects. This chain effectively runs across the segmented texts, extracting key information and producing a final, condensed summary of the combined initial summaries. Finally, this ultimate summary is displayed in markdown format to maintain clarity and formatting, providing a clear and succinct synthesis of the original articles' content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98
    },
    "id": "cjpShxq87RvJ",
    "outputId": "1baf5d03-fc50-4710-c61a-12baf74450d5"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " The Transformer model is a neural network architecture introduced in 2017 by Geoffrey Hinton and his colleagues at Google Brain. It has four main components: a self-attention mechanism, a feed-forward network, a position-wise feed-forward network, and an output layer. The self-attention mechanism helps the model learn to attend to relevant parts of the input, while the feed-forward network is used to compute the weighted sum of all the previous hidden states in the network."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "\n",
    "summary_str = \" \".join(summaries)\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "texts = text_splitter.split_text(summary_str)\n",
    "docs = [Document(page_content=t) for t in texts]\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "final_summary = chain.invoke(docs)['output_text']\n",
    "display_markdown(final_summary, raw=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
