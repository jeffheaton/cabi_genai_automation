{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whjsJasuhstV"
   },
   "source": [
    "# Introduction to Automation with LangChain, Generative AI, and Python\n",
    "**3.1: Structured Output Parser**\n",
    "* Instructor: [Jeff Heaton](https://youtube.com/@HeatonResearch), WUSTL Center for Analytics and Business Insight (CABI), [Washington University in St. Louis](https://olin.wustl.edu/faculty-and-research/research-centers/center-for-analytics-and-business-insight/index.php)\n",
    "* For more information visit the [class website](https://github.com/jeffheaton/cabi_genai_automation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "LangChain offers a diverse array of output parsers designed to efficiently extract and structure information from the outputs returned by large language models (LLMs). These output parsers play a crucial role in LangChain's architecture, typically forming integral components of what are known as LangChain chains. These chains are configurable sequences of operations that process and interact with model outputs to perform complex tasks. To facilitate the creation and management of these chains, LangChain introduces the LangChain Expression Language (LCEL).\n",
    "\n",
    "Next, we will delve into how LCEL enables the advanced creation and execution of LangChain chains, and subsequently explore how different output parsers can be utilized within these chains to achieve precise and actionable insights from LLM outputs.\n",
    "\n",
    "## LangChain Expression Language, or LCEL\n",
    "\n",
    "LangChain Expression Language, or [LCEL](https://python.langchain.com/docs/expression_language/), provides a declarative way to easily compose chains. From its inception, LCEL has supported transitioning prototypes to production without any code changes. This includes everything from simple \"prompt + LLM\" chains to highly complex chains that some users have successfully implemented with hundreds of steps in production environments. Here are several reasons why you might choose LCEL:\n",
    "\n",
    "* [First-class streaming support](https://python.langchain.com/docs/expression_language/streaming/): By building your chains with LCEL, you achieve the optimal time-to-first-token, which is the time elapsed until the first output chunk appears. In some instances, this means streaming tokens directly from an LLM to a streaming output parser, delivering parsed, incremental chunks of output at the same rate the LLM provider releases the raw tokens.\n",
    "\n",
    "* [Async support](https://python.langchain.com/docs/expression_language/interface/): Chains built with LCEL can use both synchronous APIs (for example, in your Jupyter notebook during prototyping) and asynchronous APIs (for example, in a LangServe server). This dual capability allows the same code to perform efficiently in both prototypes and production, handling many concurrent requests on the same server.\n",
    "\n",
    "* [Optimized parallel execution](https://python.langchain.com/docs/expression_language/primitives/parallel/): LCEL automatically executes parallelizable steps in your chains simultaneously, whether you are using synchronous or asynchronous interfaces, minimizing latency.\n",
    "\n",
    "* [Retries and fallbacks](https://python.langchain.com/docs/guides/productionization/fallbacks/): You can configure retries and fallbacks for any part of your LCEL chain, enhancing reliability at scale. We are also enhancing streaming support for retries and fallbacks to improve reliability without increasing latency.\n",
    "\n",
    "* [Access to intermediate results](https://python.langchain.com/docs/expression_language/interface/#async-stream-events-beta): For more complex chains, accessing intermediate results can be crucial. This feature allows end-users to see progress or helps developers debug the chain. Intermediate results are streamable and available on every LangServe server.\n",
    "\n",
    "* [Input and output schemas](https://python.langchain.com/docs/expression_language/interface/#input-schema): Every LCEL chain comes with Pydantic and JSONSchema schemas inferred from your chain's structure. These schemas are essential for validating inputs and outputs and are a core feature of LangServe.\n",
    "\n",
    "* [Seamless LangSmith tracing](https://python.langchain.com/docs/langsmith/): As chains grow in complexity, it's vital to trace each step. LCEL automatically logs all steps to LangSmith for enhanced observability and debuggability.\n",
    "\n",
    "* [Seamless LangServe deployment](https://python.langchain.com/docs/langserve/): Deploying a chain created with LCEL is straightforward using LangServe, facilitating seamless transitions from development to production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMLrKTqgSRkM"
   },
   "source": [
    "# Introduction to the StructuredOutputParser\n",
    "\n",
    "LangChain offers a variety of tools designed to help process and extract information from the output of large language models (LLMs). In this section, we will explore the capabilities of the Structured Output Parser, which is particularly useful when you need to return information across multiple fields. This parser is adept at organizing and segmenting model output into distinct categories, making the data more manageable and interpretable. Although the Pydantic/JSON parser provides a more robust and feature-rich option for handling complex data structures, the Structured Output Parser is an excellent choice for environments where computational resources are limited, or when using less powerful models. It offers a straightforward and efficient way to structure output without overwhelming the model or the system.\n",
    "\n",
    "We begin by creating a ResponseSchema for each value we wish to extract. We must describe each value. We construct the StructuredOutputParser from a list of these schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Uu5zOki19kKR"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"answer\", description=\"answer to the user's question\"),\n",
    "    ResponseSchema(\n",
    "        name=\"source\",\n",
    "        description=\"source used to answer the user's question, should be a website.\",\n",
    "    ),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dQTrDn8TuUk"
   },
   "source": [
    "We now construct a prompt template that allows both the question and formatting instructions to be specified. The schemas that we just created will generate the formatting instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mXwzsoJU9kAY"
   },
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"answer the users question as best as possible.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DDMfoBUUDjs"
   },
   "source": [
    "Before we query the LLM, let's provide a question and see how LangChang constructs the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IX9IhDJpBDNN",
    "outputId": "d2f9fcb1-a3a6-4f05-f26a-72080ac7e355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer the users question as best as possible.\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"answer\": string  // answer to the user's question\n",
      "\t\"source\": string  // source used to answer the user's question, should be a website.\n",
      "}\n",
      "```\n",
      "When was the Python programming language introduced?\n"
     ]
    }
   ],
   "source": [
    "question = \"When was the Python programming language introduced?\"\n",
    "\n",
    "print(prompt.invoke(question).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCC1bV0dUZgt"
   },
   "source": [
    "As you can see, the schemas LangChain uses the schemas to specify a JSON format to return the answer. With the answer in this JSON format, it is straightforward to parse the individual values.\n",
    "\n",
    "We now construct a chain to use the StructuredOutputParser and query the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "CWjcI5zB9j9k"
   },
   "outputs": [],
   "source": [
    "MODEL = 'mistral.mistral-7b-instruct-v0:2'\n",
    "\n",
    "# Initialize bedrock, use built in role\n",
    "model = ChatBedrock(\n",
    "        model_id=MODEL,\n",
    "        model_kwargs={\"temperature\": 0.0},\n",
    ")\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gux36efBgSR9"
   },
   "source": [
    "Now, we present a question, an answer, and a source for the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SxxA2nKN9j7B",
    "outputId": "f035418e-3968-45a7-f1a4-d79d62a8f954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'December 7, 1994', 'source': 'https://en.wikipedia.org/wiki/Python_(programming_language)'}\n"
     ]
    }
   ],
   "source": [
    "result = chain.invoke({\"question\": question})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wR5J9U9bg5Br"
   },
   "source": [
    "## Detect and Translate\n",
    "\n",
    "We will now try an example with more values. The following program accepts text in any language and will translate it into French, Spanish, and Chinese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "9snGgL4riemY"
   },
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"detected\", description=\"The language of the user's input\"),\n",
    "    ResponseSchema(name=\"spanish\", description=\"Spanish translation\"),\n",
    "    ResponseSchema(name=\"french\", description=\"French translation\"),\n",
    "    ResponseSchema(name=\"chinese\", description=\"Chinese translation\"),\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "prompt = PromptTemplate(\n",
    "    template=\"translate into the requested languages.\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# Initialize bedrock, use built in role\n",
    "model = ChatBedrock(\n",
    "        model_id=MODEL,\n",
    "        model_kwargs={\"temperature\": 0.0},\n",
    ")\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sa6_dAk2ugre"
   },
   "source": [
    "We begin by trying an English sentence and observe it translated into the other three languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9wKUJR3tiYP",
    "outputId": "2e7a2aa4-8758-4135-d2e3-d834ccfcf44a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detected': 'en', 'spanish': 'Cuándo se introdujo el lenguaje de programación Python?', 'french': 'Quand le langage de programmation Python a-t-il été introduit?', 'chinese': 'Python编程语言何时出现？'}\n"
     ]
    }
   ],
   "source": [
    "question = \"When was the Python programming language introduced?\"\n",
    "result = chain.invoke({\"question\": question})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7QKN6j8vNGF"
   },
   "source": [
    "We also observe that it can detect and translate Swahili."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pDtvdS6htpMg",
    "outputId": "80957b38-8ad2-4586-bcc9-6f8045462ca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'detected': 'swahili', 'spanish': 'No entiendo quién espera', 'french': 'Je ne comprends pas qui attend', 'chinese': '我不理解谁在等'}\n"
     ]
    }
   ],
   "source": [
    "question = \"Sijui nini kinaendelea?\"\n",
    "result = chain.invoke({\"question\": question})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
