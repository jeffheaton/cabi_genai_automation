{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "euOZxlIMhstX"
   },
   "source": [
    "# Introduction to Automation with LangChain, Generative AI, and Python\n",
    "**1.3: Code Generation Handling Revision Prompts**\n",
    "* Instructor: [Jeff Heaton](https://youtube.com/@HeatonResearch), WUSTL Center for Analytics and Business Insight (CABI), [Washington University in St. Louis](https://olin.wustl.edu/faculty-and-research/research-centers/center-for-analytics-and-business-insight/index.php)\n",
    "* For more information visit the [class website](https://github.com/jeffheaton/cabi_genai_automation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC9A-LaYhsta"
   },
   "source": [
    "Previously, we just sent one prompt to the LLM, which generated code. It is possible to perform this code more conversationally. In this module, we will see how to converse with the LLM to request changes to outputted code and even help the LLM to produce a more accurate model.\n",
    "\n",
    "We will also see that it might be beneficial to recreate your conversation as one single prompt that generates the final result. Keeping track of one prompt, rather than a conversation, that created your final code is more maintainable.\n",
    "\n",
    "## Conversational Code Generation\n",
    "\n",
    "We will introduce a more advanced code generation function that allows you to start the conversation to generate code and follow up with additional prompts if needed.\n",
    "\n",
    "In future modules, we will see how to create chatbots similar to this one. We will use the code I provided to generate your code for now. This generator uses a system prompt that requests that the generated code conform to the following:\n",
    "\n",
    "* Imports should be sorted\n",
    "* Code should conform to PEP-8 formatting\n",
    "* Do not mix uncompilable notes with code\n",
    "* Add comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TMF-rtxgRAea"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts.chat import PromptTemplate\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "MODEL = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "TEMPLATE = \"\"\"The following is a friendly conversation between a human and an\n",
    "AI to generate Python code. If you have notes about the code, place them before\n",
    "the code. Any nots about execution should follow the code. If you do mix any\n",
    "notes with the code, make them comments. Add proper comments to the code.\n",
    "Sort imports and follow PEP-8 formatting.\n",
    "\n",
    "Current conversation:\n",
    "{history}\n",
    "Human: {input}\n",
    "Code Assistant:\"\"\"\n",
    "PROMPT_TEMPLATE = PromptTemplate(input_variables=[\"history\", \"input\"], template=TEMPLATE)\n",
    "\n",
    "def start_conversation():\n",
    "    # Initialize bedrock, use built in role\n",
    "    llm = ChatBedrock(\n",
    "        model_id=MODEL,\n",
    "        model_kwargs={\"temperature\": 0.0},\n",
    "    )\n",
    "\n",
    "    # Initialize memory and conversation\n",
    "    memory = ConversationBufferWindowMemory()\n",
    "    conversation = ConversationChain(\n",
    "        prompt=PROMPT_TEMPLATE,\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    return conversation\n",
    "\n",
    "def generate_code(conversation, prompt):\n",
    "    print(\"Model response:\")\n",
    "    output = conversation.invoke(prompt)\n",
    "    display_markdown(output['response'], raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClPhLkGldhPt"
   },
   "source": [
    "## First Attempt at an XOR Approximator\n",
    "\n",
    "We will construct a prompt that requests the LLM to generate a PyTorch neural network to approximate the [Exclusive Or](https://en.wikipedia.org/wiki/Exclusive_or). The truth table for the Exclusive Or (XOR) function is provided here:\n",
    "\n",
    "```\n",
    "0 XOR 0 = 0\n",
    "1 XOR 0 = 1\n",
    "0 XOR 1 = 1\n",
    "1 XOR 1 = 0\n",
    "```\n",
    "\n",
    "If given data, neural networks can learn to approximate functions, so let's create a PyTorch neural network to approximate the XOR function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ydaqwgRiH4D6",
    "outputId": "9ef9cde0-190c-4c38-c2a2-f80e65bef76b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "\n",
       "# Define the XOR dataset\n",
       "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
       "\n",
       "# Define the neural network model\n",
       "model = nn.Sequential(\n",
       "    nn.Linear(2, 2),  # Input layer with 2 inputs and 2 outputs\n",
       "    nn.Sigmoid(),     # Activation function\n",
       "    nn.Linear(2, 1),  # Output layer with 1 output\n",
       "    nn.Sigmoid()      # Activation function for output\n",
       ")\n",
       "\n",
       "# Define the loss function and optimizer\n",
       "criterion = nn.BCELoss()\n",
       "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
       "\n",
       "# Train the model\n",
       "num_epochs = 10000\n",
       "for epoch in range(num_epochs):\n",
       "    # Forward pass\n",
       "    outputs = model(X)\n",
       "    loss = criterion(outputs, y)\n",
       "\n",
       "    # Backward pass and optimization\n",
       "    optimizer.zero_grad()\n",
       "    loss.backward()\n",
       "    optimizer.step()\n",
       "\n",
       "    # Print the loss every 1000 epochs\n",
       "    if (epoch + 1) % 1000 == 0:\n",
       "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
       "\n",
       "# Test the model\n",
       "with torch.no_grad():\n",
       "    outputs = model(X)\n",
       "    predicted = (outputs > 0.5).float()\n",
       "    accuracy = (predicted == y).float().mean()\n",
       "    print(f'Accuracy: {accuracy.item() * 100:.2f}%')\n",
       "```\n",
       "\n",
       "Notes:\n",
       "- This code defines a simple neural network with two linear layers and sigmoid activation functions to learn the XOR function.\n",
       "- The XOR dataset is defined as a tensor with input vectors and corresponding target outputs.\n",
       "- The model is trained using the binary cross-entropy loss and stochastic gradient descent optimizer.\n",
       "- The training loop runs for 10,000 epochs, and the loss is printed every 1,000 epochs.\n",
       "- After training, the model's accuracy is evaluated on the XOR dataset.\n",
       "\n",
       "Execution notes:\n",
       "- The code should output the loss value every 1,000 epochs during training.\n",
       "- After training, the code will print the final accuracy of the model on the XOR dataset.\n",
       "- The final accuracy should be close to 100%, indicating that the model has learned the XOR function."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conversation = start_conversation()\n",
    "generate_code(conversation, \"\"\"Write Python code to learn the XOR function with PyTorch.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hPqM7BslmM7"
   },
   "source": [
    "# Requesting a Change to Generated Code\n",
    "\n",
    "If you've taken my other course, you will know I prefer PyTorch sequences over extending the nn.Module class, at least for simple neural networks like an XOR approximator. LLMs do not share this opinion. However, the LLM will gladly humor me and generate a sequence. Here, I provide an additional prompt to request this rather than resubmitting a modified version of my first prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 948
    },
    "id": "lqnDZhc4OVU6",
    "outputId": "8ef3e8c4-68c7-4001-8708-efaceead6102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, here's the Python code using PyTorch to learn the XOR function with 4 hidden neurons, using the Adam optimizer, and 20K training epochs, but this time extending the `nn.Module` class instead of using a sequence of PyTorch operations.\n",
       "\n",
       "```python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "\n",
       "# Define the XOR model\n",
       "class XORModel(nn.Module):\n",
       "    def __init__(self):\n",
       "        super(XORModel, self).__init__()\n",
       "        self.hidden = nn.Linear(2, 4)  # Input to hidden layer\n",
       "        self.output = nn.Linear(4, 1)  # Hidden to output layer\n",
       "\n",
       "    def forward(self, x):\n",
       "        hidden = torch.relu(self.hidden(x))\n",
       "        output = self.output(hidden)\n",
       "        return output\n",
       "\n",
       "# Define the training data\n",
       "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
       "\n",
       "# Initialize the model\n",
       "model = XORModel()\n",
       "\n",
       "# Define the optimizer\n",
       "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
       "\n",
       "# Training loop\n",
       "for epoch in range(20000):\n",
       "    # Forward pass\n",
       "    output = model(X)\n",
       "\n",
       "    # Compute the loss\n",
       "    loss = F.mse_loss(output, y)\n",
       "\n",
       "    # Backward pass and optimization\n",
       "    optimizer.zero_grad()\n",
       "    loss.backward()\n",
       "    optimizer.step()\n",
       "\n",
       "    # Print the loss every 1000 epochs\n",
       "    if (epoch + 1) % 1000 == 0:\n",
       "        print(f\"Epoch [{epoch + 1}/20000], Loss: {loss.item():.4f}\")\n",
       "\n",
       "# Test the model\n",
       "with torch.no_grad():\n",
       "    print(\"\\nOutput for [0, 0]:\", torch.round(torch.sigmoid(model(torch.tensor([0, 0])))))\n",
       "    print(\"Output for [0, 1]:\", torch.round(torch.sigmoid(model(torch.tensor([0, 1])))))\n",
       "    print(\"Output for [1, 0]:\", torch.round(torch.sigmoid(model(torch.tensor([1, 0])))))\n",
       "    print(\"Output for [1, 1]:\", torch.round(torch.sigmoid(model(torch.tensor([1, 1])))))\n",
       "```\n",
       "\n",
       "Notes:\n",
       "- The `XORModel` class is defined by extending the `nn.Module` class.\n",
       "- The `__init__` method initializes the hidden and output layers using `nn.Linear`.\n",
       "- The `forward` method defines the forward pass of the model, applying ReLU activation to the hidden layer.\n",
       "- The training data `X` and `y` are defined as before.\n",
       "- The model is initialized using `XORModel()`.\n",
       "- The Adam optimizer is used with a learning rate of 0.01, and the model parameters are passed to the optimizer.\n",
       "- The training loop runs for 20,000 epochs.\n",
       "- In each epoch, the forward pass computes the output using `model(X)`.\n",
       "- The loss is computed using the mean squared error (MSE) loss function.\n",
       "- The backward pass computes the gradients, and the optimizer updates the model parameters.\n",
       "- The loss is printed every 1,000 epochs.\n",
       "- After training, the model is tested on the four input combinations of the XOR function, and the outputs are rounded to 0 or 1 using `torch.round` and the sigmoid activation function.\n",
       "- The `with torch.no_grad()` context is used to disable gradient computation during testing.\n",
       "\n",
       "Execution notes:\n",
       "- The code should learn the XOR function reasonably well after 20,000 training epochs.\n",
       "- The final outputs for the four input combinations should be close to the expected XOR outputs.\n",
       "- The loss should decrease over the training epochs, indicating that the model is learning the XOR function."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_code(conversation, \"\"\"\n",
    "Could extend the nn.Module class, rather than use a PyTorch sequence array rather than defining use a sequence?\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7U1Bx3Wbje1"
   },
   "source": [
    "# Testing the Generated Code\n",
    "\n",
    "LLMs are not overachievers; they will implement the code you ask for and not provide much more. When we run the XOR approximator's first version, the results are only sometimes accurate, especially if we run the program multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdJNKLVhRcD3",
    "outputId": "7e690772-13d7-4ec7-813a-64b426bb8855"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/20000], Loss: 0.1250\n",
      "Epoch [2000/20000], Loss: 0.1250\n",
      "Epoch [3000/20000], Loss: 0.1250\n",
      "Epoch [4000/20000], Loss: 0.1250\n",
      "Epoch [5000/20000], Loss: 0.1250\n",
      "Epoch [6000/20000], Loss: 0.1250\n",
      "Epoch [7000/20000], Loss: 0.1250\n",
      "Epoch [8000/20000], Loss: 0.1250\n",
      "Epoch [9000/20000], Loss: 0.1250\n",
      "Epoch [10000/20000], Loss: 0.1250\n",
      "Epoch [11000/20000], Loss: 0.1250\n",
      "Epoch [12000/20000], Loss: 0.1250\n",
      "Epoch [13000/20000], Loss: 0.1250\n",
      "Epoch [14000/20000], Loss: 0.1250\n",
      "Epoch [15000/20000], Loss: 0.1250\n",
      "Epoch [16000/20000], Loss: 0.1250\n",
      "Epoch [17000/20000], Loss: 0.1250\n",
      "Epoch [18000/20000], Loss: 0.1250\n",
      "Epoch [19000/20000], Loss: 0.1250\n",
      "Epoch [20000/20000], Loss: 0.1250\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOutput for [0, 0]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)))\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput for [0, 1]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(model(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])))))\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput for [1, 0]:\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mround(torch\u001b[38;5;241m.\u001b[39msigmoid(model(torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m])))))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m, in \u001b[0;36mXORModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     14\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(hidden)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the XOR model\n",
    "class XORModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORModel, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 4)  # Input to hidden layer\n",
    "        self.output = nn.Linear(4, 1)  # Hidden to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.relu(self.hidden(x))\n",
    "        output = self.output(hidden)\n",
    "        return output\n",
    "\n",
    "# Define the training data\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Initialize the model\n",
    "model = XORModel()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20000):\n",
    "    # Forward pass\n",
    "    output = model(X)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = F.mse_loss(output, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss every 1000 epochs\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/20000], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    print(\"\\nOutput for [0, 0]:\", torch.round(torch.sigmoid(model(torch.tensor([0, 0])))))\n",
    "    print(\"Output for [0, 1]:\", torch.round(torch.sigmoid(model(torch.tensor([0, 1])))))\n",
    "    print(\"Output for [1, 0]:\", torch.round(torch.sigmoid(model(torch.tensor([1, 0])))))\n",
    "    print(\"Output for [1, 1]:\", torch.round(torch.sigmoid(model(torch.tensor([1, 1])))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jY-NmeRAoPVr"
   },
   "source": [
    "If you receive an error or the output is not exactly what you like, it is effective to provide that output and any errors to the LLM. Here, we provide the output and ask the LLM if that seems correct. Sometimes, the LLM may insist that the output is correct, so you must \"debate\" the LLM, providing additional details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bzkCRwx_RxxI",
    "outputId": "e9c3628f-97e4-4c56-9082-ec38719f5f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "You're right, the output you provided shows that the model is not learning the XOR function correctly. All the outputs are 1, which is incorrect for the XOR function.\n",
       "\n",
       "After reviewing the code, I believe the issue is that we are using the `nn.Linear` module with integer inputs (`torch.long`), which is not recommended. The `nn.Linear` module expects floating-point inputs and weights, and using integer inputs can lead to unexpected behavior.\n",
       "\n",
       "Here's the corrected code that should work for learning the XOR function:\n",
       "\n",
       "```python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "\n",
       "# Define the XOR model\n",
       "class XORModel(nn.Module):\n",
       "    def __init__(self):\n",
       "        super(XORModel, self).__init__()\n",
       "        self.hidden = nn.Linear(2, 4)  # Input to hidden layer\n",
       "        self.output = nn.Linear(4, 1)  # Hidden to output layer\n",
       "\n",
       "    def forward(self, x):\n",
       "        hidden = torch.relu(self.hidden(x))\n",
       "        output = self.output(hidden)\n",
       "        return output\n",
       "\n",
       "# Define the training data\n",
       "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
       "\n",
       "# Initialize the model\n",
       "model = XORModel()\n",
       "\n",
       "# Define the optimizer\n",
       "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
       "\n",
       "# Training loop\n",
       "for epoch in range(20000):\n",
       "    # Forward pass\n",
       "    output = model(X)\n",
       "\n",
       "    # Compute the loss\n",
       "    loss = F.mse_loss(output, y)\n",
       "\n",
       "    # Backward pass and optimization\n",
       "    optimizer.zero_grad()\n",
       "    loss.backward()\n",
       "    optimizer.step()\n",
       "\n",
       "    # Print the loss every 1000 epochs\n",
       "    if (epoch + 1) % 1000 == 0:\n",
       "        print(f\"Epoch [{epoch + 1}/20000], Loss: {loss.item():.4f}\")\n",
       "\n",
       "# Test the model\n",
       "with torch.no_grad():\n",
       "    print(\"\\nOutput for [0, 0]:\", torch.round(torch.sigmoid(model(torch.tensor([0, 0], dtype=torch.float32)))))\n",
       "    print(\"Output for [0, 1]:\", torch.round(torch.sigmoid(model(torch.tensor([0, 1], dtype=torch.float32)))))\n",
       "    print(\"Output for [1, 0]:\", torch.round(torch.sigmoid(model(torch.tensor([1, 0], dtype=torch.float32)))))\n",
       "    print(\"Output for [1, 1]:\", torch.round(torch.sigmoid(model(torch.tensor([1, 1], dtype=torch.float32)))))\n",
       "```\n",
       "\n",
       "The changes made are:\n",
       "\n",
       "1. The input tensor `X` and the target tensor `y` are defined with the `torch.float32` data type.\n",
       "2. During testing, the input tensors are also created with the `torch.float32` data type.\n",
       "\n",
       "With these changes, the model should learn the XOR function correctly, and the outputs should be close to the expected XOR outputs after 20,000 training epochs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_code(conversation, \"\"\"\n",
    "I do not think this is correct.\n",
    "\n",
    "Output for [0, 0]: tensor([1.])\n",
    "Output for [0, 1]: tensor([1.])\n",
    "Output for [1, 0]: tensor([1.])\n",
    "Output for [1, 1]: tensor([1.])\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r2Qpe11cWwm"
   },
   "source": [
    "## Test the Improved Version\n",
    "\n",
    "We now receive much more accurate output when we test the neural network provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V19QyaROSUP1",
    "outputId": "8e8993aa-6d11-4788-b6e5-3d291c8097de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1000/20000], Loss: 0.0000\n",
      "Epoch [2000/20000], Loss: 0.0000\n",
      "Epoch [3000/20000], Loss: 0.0000\n",
      "Epoch [4000/20000], Loss: 0.0000\n",
      "Epoch [5000/20000], Loss: 0.0000\n",
      "Epoch [6000/20000], Loss: 0.0000\n",
      "Epoch [7000/20000], Loss: 0.0000\n",
      "Epoch [8000/20000], Loss: 0.0000\n",
      "Epoch [9000/20000], Loss: 0.0000\n",
      "Epoch [10000/20000], Loss: 0.0000\n",
      "Epoch [11000/20000], Loss: 0.0000\n",
      "Epoch [12000/20000], Loss: 0.0000\n",
      "Epoch [13000/20000], Loss: 0.0000\n",
      "Epoch [14000/20000], Loss: 0.0000\n",
      "Epoch [15000/20000], Loss: 0.0000\n",
      "Epoch [16000/20000], Loss: 0.0000\n",
      "Epoch [17000/20000], Loss: 0.0000\n",
      "Epoch [18000/20000], Loss: 0.0000\n",
      "Epoch [19000/20000], Loss: 0.0000\n",
      "Epoch [20000/20000], Loss: 0.0000\n",
      "\n",
      "Output for [0, 0]: tensor([0.])\n",
      "Output for [0, 1]: tensor([1.])\n",
      "Output for [1, 0]: tensor([1.])\n",
      "Output for [1, 1]: tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the XOR model\n",
    "class XORModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORModel, self).__init__()\n",
    "        self.hidden = nn.Linear(2, 4)  # Input to hidden layer\n",
    "        self.output = nn.Linear(4, 1)  # Hidden to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = torch.relu(self.hidden(x))\n",
    "        output = self.output(hidden)\n",
    "        return output\n",
    "\n",
    "# Define the training data\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Initialize the model\n",
    "model = XORModel()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20000):\n",
    "    # Forward pass\n",
    "    output = model(X)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = F.mse_loss(output, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss every 1000 epochs\n",
    "    if (epoch + 1) % 1000 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/20000], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    print(\"\\nOutput for [0, 0]:\", torch.round(torch.sigmoid(model(torch.tensor([0, 0], dtype=torch.float32)))))\n",
    "    print(\"Output for [0, 1]:\", torch.round(torch.sigmoid(model(torch.tensor([0, 1], dtype=torch.float32)))))\n",
    "    print(\"Output for [1, 0]:\", torch.round(torch.sigmoid(model(torch.tensor([1, 0], dtype=torch.float32)))))\n",
    "    print(\"Output for [1, 1]:\", torch.round(torch.sigmoid(model(torch.tensor([1, 1], dtype=torch.float32)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCAmOVHEpAjf"
   },
   "source": [
    "## Combining the Conversation into a Single Prompt\n",
    "\n",
    "We should combine this entire conversation into a single prompt, especially if we wish to save the prompt along with the code. We can request the LLM to create this combined prompt for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "MKMcmXTNSiFw",
    "outputId": "4f81e0cd-57b3-4581-e0e9-18d129663384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Sure, here's a single prompt that covers all the details we discussed and would have resulted in the correct code output:\n",
       "\n",
       "\"Write a Python code using PyTorch to learn the XOR function with 4 hidden neurons, using the Adam optimizer, and 20K training epochs. Extend the `nn.Module` class to define the model, and ensure that the input data and model weights are compatible floating-point data types (e.g., `torch.float32`). Include proper comments, follow PEP-8 formatting, and sort imports. After training, test the model on the four input combinations of the XOR function and print the rounded outputs.\"\n",
       "\n",
       "This prompt covers the following key points:\n",
       "\n",
       "1. Use PyTorch to learn the XOR function.\n",
       "2. Define the model by extending the `nn.Module` class.\n",
       "3. Use 4 hidden neurons in the model.\n",
       "4. Use the Adam optimizer for training.\n",
       "5. Train for 20,000 epochs.\n",
       "6. Ensure that the input data and model weights are compatible floating-point data types (e.g., `torch.float32`).\n",
       "7. Include proper comments and follow PEP-8 formatting.\n",
       "8. Sort imports.\n",
       "9. After training, test the model on the four input combinations of the XOR function.\n",
       "10. Print the rounded outputs for the four input combinations.\n",
       "\n",
       "By following this prompt, the resulting code should correctly learn the XOR function using PyTorch and provide the expected outputs for the four input combinations."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_code(conversation, \"\"\"Okay, that is great, can you suggest a single\n",
    "prompt that would have resulted in this last code output? Which covers\n",
    "all of the details we discussed.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mw-QDu7Rpo_T"
   },
   "source": [
    "The LLM's attempt at a consoldated prompt is incomplete. It skips several important details and does not provide precise requirements. I will manually make some improvements, which you can see here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1TzyiT6aS7ej",
    "outputId": "13c3d633-d5c7-4c57-8787-581ed09b2618"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "import torch\n",
       "import torch.nn as nn\n",
       "import torch.nn.functional as F\n",
       "import torch.optim as optim\n",
       "\n",
       "# Define the XOR dataset\n",
       "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
       "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
       "\n",
       "# Define the neural network model\n",
       "class XORModel(nn.Module):\n",
       "    def __init__(self):\n",
       "        super(XORModel, self).__init__()\n",
       "        self.fc1 = nn.Linear(2, 4)  # Input layer (2 inputs) to hidden layer (4 neurons)\n",
       "        self.fc2 = nn.Linear(4, 1)  # Hidden layer (4 neurons) to output layer (1 output)\n",
       "\n",
       "    def forward(self, x):\n",
       "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the hidden layer\n",
       "        x = self.fc2(x)  # Apply linear transformation to the output layer\n",
       "        return torch.sigmoid(x)  # Apply sigmoid activation to the output\n",
       "\n",
       "# Create an instance of the model\n",
       "model = XORModel()\n",
       "\n",
       "# Define the loss function and optimizer\n",
       "criterion = nn.BCELoss()\n",
       "optimizer = optim.Adam(model.parameters())\n",
       "\n",
       "# Training loop\n",
       "for epoch in range(20000):\n",
       "    # Forward pass\n",
       "    outputs = model(X)\n",
       "    loss = criterion(outputs, y)\n",
       "\n",
       "    # Backward pass and optimization\n",
       "    optimizer.zero_grad()\n",
       "    loss.backward()\n",
       "    optimizer.step()\n",
       "\n",
       "# Test the model\n",
       "with torch.no_grad():\n",
       "    for inputs in X:\n",
       "        output = model(inputs.unsqueeze(0))\n",
       "        print(f\"Input: {inputs.tolist()}, Output: {round(output.item())}\")\n",
       "```\n",
       "\n",
       "Notes on execution:\n",
       "\n",
       "1. The code defines the XOR dataset as tensors `X` and `y`.\n",
       "2. The `XORModel` class extends `nn.Module` and defines the neural network architecture with an input layer, a hidden layer with 4 neurons, and an output layer.\n",
       "3. The `forward` method of the `XORModel` class defines the forward pass of the neural network, applying ReLU activation to the hidden layer and sigmoid activation to the output layer.\n",
       "4. An instance of the `XORModel` is created.\n",
       "5. The loss function (`nn.BCELoss`) and optimizer (`optim.Adam`) are defined.\n",
       "6. The training loop runs for 20,000 epochs, performing forward and backward passes, and updating the model parameters using the optimizer.\n",
       "7. After training, the model is tested on the four input combinations of the XOR function, and the rounded outputs are printed."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Start a new conversation\n",
    "conversation = start_conversation()\n",
    "generate_code(conversation, \"\"\"\n",
    "Write a Python code using PyTorch to learn the XOR function with 4 hidden neurons, using the Adam optimizer, and 20K training epochs. Extend the nn.Module class to define the model, and ensure that the input data and model weights are compatible floating-point data types (e.g., torch.float32). Include proper comments, follow PEP-8 formatting, and sort imports. After training, test the model on the four input combinations of the XOR function and print the rounded outputs.\"\n",
    "\n",
    "This prompt covers the following key points:\n",
    "\n",
    "Use PyTorch to learn the XOR function.\n",
    "Define the model by extending the nn.Module class.\n",
    "Use 4 hidden neurons in the model.\n",
    "Use the Adam optimizer for training.\n",
    "Train for 20,000 epochs.\n",
    "Ensure that the input data and model weights are compatible floating-point data types (e.g., torch.float32).\n",
    "Include proper comments and follow PEP-8 formatting.\n",
    "Sort imports.\n",
    "After training, test the model on the four input combinations of the XOR function.\n",
    "Print the rounded outputs for the four input combinations.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EiMd01ATosU"
   },
   "source": [
    "## Test the Final Prompt\n",
    "\n",
    "Now, we test the final prompt. My prompt produces an acceptable result, but there are some opportunities for improvement. You can specify the exact format for the output. For example, sometimes code is generated to round the results, but other times it is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fx0_Al1kTmh7",
    "outputId": "131bc687-76a4-4ec4-cb62-0ef08e815be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0.0, 0.0], Output: 0\n",
      "Input: [0.0, 1.0], Output: 1\n",
      "Input: [1.0, 0.0], Output: 1\n",
      "Input: [1.0, 1.0], Output: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the XOR dataset\n",
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "y = torch.tensor([[0], [1], [1], [0]], dtype=torch.float32)\n",
    "\n",
    "# Define the neural network model\n",
    "class XORModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 4)  # Input layer (2 inputs) to hidden layer (4 neurons)\n",
    "        self.fc2 = nn.Linear(4, 1)  # Hidden layer (4 neurons) to output layer (1 output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  # Apply ReLU activation to the hidden layer\n",
    "        x = self.fc2(x)  # Apply linear transformation to the output layer\n",
    "        return torch.sigmoid(x)  # Apply sigmoid activation to the output\n",
    "\n",
    "# Create an instance of the model\n",
    "model = XORModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(20000):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Test the model\n",
    "with torch.no_grad():\n",
    "    for inputs in X:\n",
    "        output = model(inputs.unsqueeze(0))\n",
    "        print(f\"Input: {inputs.tolist()}, Output: {round(output.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
